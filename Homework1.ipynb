{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HM1: Logistic Regression.\n",
    "\n",
    "### Name: [Shadi Ebadi]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For this assignment, you will build 6 models. You need to train Logistic Regression/Regularized Logistic Regression each with Batch Gradient Descent, Stochastic Gradient Descent and Mini Batch Gradient Descent. Also you should plot their objective values versus epochs and compare their training and testing accuracies. You will need to tune the parameters a little bit to obtain reasonable results.\n",
    "\n",
    "#### You do not have to follow the following procedure. You may implement your own functions and methods, but you need to show your results and plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sheba\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\sheba\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "# Load Packages\n",
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "np.random.seed(42)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data processing\n",
    "\n",
    "- Download the Breast Cancer dataset from canvas or from https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)\n",
    "- Load the data.\n",
    "- Preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>926424</td>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>926682</td>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>926954</td>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>927241</td>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>92751</td>\n",
       "      <td>B</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0      842302         M        17.99         10.38          122.80     1001.0   \n",
       "1      842517         M        20.57         17.77          132.90     1326.0   \n",
       "2    84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3    84348301         M        11.42         20.38           77.58      386.1   \n",
       "4    84358402         M        20.29         14.34          135.10     1297.0   \n",
       "..        ...       ...          ...           ...             ...        ...   \n",
       "564    926424         M        21.56         22.39          142.00     1479.0   \n",
       "565    926682         M        20.13         28.25          131.20     1261.0   \n",
       "566    926954         M        16.60         28.08          108.30      858.1   \n",
       "567    927241         M        20.60         29.33          140.10     1265.0   \n",
       "568     92751         B         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0            0.11840           0.27760         0.30010              0.14710   \n",
       "1            0.08474           0.07864         0.08690              0.07017   \n",
       "2            0.10960           0.15990         0.19740              0.12790   \n",
       "3            0.14250           0.28390         0.24140              0.10520   \n",
       "4            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "564          0.11100           0.11590         0.24390              0.13890   \n",
       "565          0.09780           0.10340         0.14400              0.09791   \n",
       "566          0.08455           0.10230         0.09251              0.05302   \n",
       "567          0.11780           0.27700         0.35140              0.15200   \n",
       "568          0.05263           0.04362         0.00000              0.00000   \n",
       "\n",
       "     ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0    ...          17.33           184.60      2019.0           0.16220   \n",
       "1    ...          23.41           158.80      1956.0           0.12380   \n",
       "2    ...          25.53           152.50      1709.0           0.14440   \n",
       "3    ...          26.50            98.87       567.7           0.20980   \n",
       "4    ...          16.67           152.20      1575.0           0.13740   \n",
       "..   ...            ...              ...         ...               ...   \n",
       "564  ...          26.40           166.10      2027.0           0.14100   \n",
       "565  ...          38.25           155.00      1731.0           0.11660   \n",
       "566  ...          34.12           126.70      1124.0           0.11390   \n",
       "567  ...          39.42           184.60      1821.0           0.16500   \n",
       "568  ...          30.37            59.16       268.6           0.08996   \n",
       "\n",
       "     compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0              0.66560           0.7119                0.2654          0.4601   \n",
       "1              0.18660           0.2416                0.1860          0.2750   \n",
       "2              0.42450           0.4504                0.2430          0.3613   \n",
       "3              0.86630           0.6869                0.2575          0.6638   \n",
       "4              0.20500           0.4000                0.1625          0.2364   \n",
       "..                 ...              ...                   ...             ...   \n",
       "564            0.21130           0.4107                0.2216          0.2060   \n",
       "565            0.19220           0.3215                0.1628          0.2572   \n",
       "566            0.30940           0.3403                0.1418          0.2218   \n",
       "567            0.86810           0.9387                0.2650          0.4087   \n",
       "568            0.06444           0.0000                0.0000          0.2871   \n",
       "\n",
       "     fractal_dimension_worst  Unnamed: 32  \n",
       "0                    0.11890          NaN  \n",
       "1                    0.08902          NaN  \n",
       "2                    0.08758          NaN  \n",
       "3                    0.17300          NaN  \n",
       "4                    0.07678          NaN  \n",
       "..                       ...          ...  \n",
       "564                  0.07115          NaN  \n",
       "565                  0.06637          NaN  \n",
       "566                  0.07820          NaN  \n",
       "567                  0.12400          NaN  \n",
       "568                  0.07039          NaN  \n",
       "\n",
       "[569 rows x 33 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r\"C:\\Users\\sheba\\Desktop\\stevens_semester2\\deep learning/data.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Examine and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some columns may not be useful for the model (For example, the first column contains ID number which may be irrelavant). \n",
    "# You need to get rid of the ID number feature.\n",
    "# Also you should transform target labels in the second column from 'B' and 'M' to 1 and -1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                           0\n",
      "diagnosis                    0\n",
      "radius_mean                  0\n",
      "texture_mean                 0\n",
      "perimeter_mean               0\n",
      "area_mean                    0\n",
      "smoothness_mean              0\n",
      "compactness_mean             0\n",
      "concavity_mean               0\n",
      "concave points_mean          0\n",
      "symmetry_mean                0\n",
      "fractal_dimension_mean       0\n",
      "radius_se                    0\n",
      "texture_se                   0\n",
      "perimeter_se                 0\n",
      "area_se                      0\n",
      "smoothness_se                0\n",
      "compactness_se               0\n",
      "concavity_se                 0\n",
      "concave points_se            0\n",
      "symmetry_se                  0\n",
      "fractal_dimension_se         0\n",
      "radius_worst                 0\n",
      "texture_worst                0\n",
      "perimeter_worst              0\n",
      "area_worst                   0\n",
      "smoothness_worst             0\n",
      "compactness_worst            0\n",
      "concavity_worst              0\n",
      "concave points_worst         0\n",
      "symmetry_worst               0\n",
      "fractal_dimension_worst      0\n",
      "Unnamed: 32                569\n",
      "dtype: int64\n",
      "id                           int64\n",
      "diagnosis                   object\n",
      "radius_mean                float64\n",
      "texture_mean               float64\n",
      "perimeter_mean             float64\n",
      "area_mean                  float64\n",
      "smoothness_mean            float64\n",
      "compactness_mean           float64\n",
      "concavity_mean             float64\n",
      "concave points_mean        float64\n",
      "symmetry_mean              float64\n",
      "fractal_dimension_mean     float64\n",
      "radius_se                  float64\n",
      "texture_se                 float64\n",
      "perimeter_se               float64\n",
      "area_se                    float64\n",
      "smoothness_se              float64\n",
      "compactness_se             float64\n",
      "concavity_se               float64\n",
      "concave points_se          float64\n",
      "symmetry_se                float64\n",
      "fractal_dimension_se       float64\n",
      "radius_worst               float64\n",
      "texture_worst              float64\n",
      "perimeter_worst            float64\n",
      "area_worst                 float64\n",
      "smoothness_worst           float64\n",
      "compactness_worst          float64\n",
      "concavity_worst            float64\n",
      "concave points_worst       float64\n",
      "symmetry_worst             float64\n",
      "fractal_dimension_worst    float64\n",
      "Unnamed: 32                float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Missing values\n",
    "print(data.isnull().sum()) \n",
    "print(data.dtypes)\n",
    "data = data.drop('id', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('Unnamed: 32', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                           int64\n",
      "diagnosis                   object\n",
      "radius_mean                float64\n",
      "texture_mean               float64\n",
      "perimeter_mean             float64\n",
      "area_mean                  float64\n",
      "smoothness_mean            float64\n",
      "compactness_mean           float64\n",
      "concavity_mean             float64\n",
      "concave points_mean        float64\n",
      "symmetry_mean              float64\n",
      "fractal_dimension_mean     float64\n",
      "radius_se                  float64\n",
      "texture_se                 float64\n",
      "perimeter_se               float64\n",
      "area_se                    float64\n",
      "smoothness_se              float64\n",
      "compactness_se             float64\n",
      "concavity_se               float64\n",
      "concave points_se          float64\n",
      "symmetry_se                float64\n",
      "fractal_dimension_se       float64\n",
      "radius_worst               float64\n",
      "texture_worst              float64\n",
      "perimeter_worst            float64\n",
      "area_worst                 float64\n",
      "smoothness_worst           float64\n",
      "compactness_worst          float64\n",
      "concavity_worst            float64\n",
      "concave points_worst       float64\n",
      "symmetry_worst             float64\n",
      "fractal_dimension_worst    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sheba\\AppData\\Local\\Temp/ipykernel_18844/3977305466.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['diagnosis'] = data['diagnosis'].replace({'M': 1, 'B': -1})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>1</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>1</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>1</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>1</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>-1</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0            1        17.99         10.38          122.80     1001.0   \n",
       "1            1        20.57         17.77          132.90     1326.0   \n",
       "2            1        19.69         21.25          130.00     1203.0   \n",
       "3            1        11.42         20.38           77.58      386.1   \n",
       "4            1        20.29         14.34          135.10     1297.0   \n",
       "..         ...          ...           ...             ...        ...   \n",
       "564          1        21.56         22.39          142.00     1479.0   \n",
       "565          1        20.13         28.25          131.20     1261.0   \n",
       "566          1        16.60         28.08          108.30      858.1   \n",
       "567          1        20.60         29.33          140.10     1265.0   \n",
       "568         -1         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0            0.11840           0.27760         0.30010              0.14710   \n",
       "1            0.08474           0.07864         0.08690              0.07017   \n",
       "2            0.10960           0.15990         0.19740              0.12790   \n",
       "3            0.14250           0.28390         0.24140              0.10520   \n",
       "4            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "564          0.11100           0.11590         0.24390              0.13890   \n",
       "565          0.09780           0.10340         0.14400              0.09791   \n",
       "566          0.08455           0.10230         0.09251              0.05302   \n",
       "567          0.11780           0.27700         0.35140              0.15200   \n",
       "568          0.05263           0.04362         0.00000              0.00000   \n",
       "\n",
       "     symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0           0.2419  ...        25.380          17.33           184.60   \n",
       "1           0.1812  ...        24.990          23.41           158.80   \n",
       "2           0.2069  ...        23.570          25.53           152.50   \n",
       "3           0.2597  ...        14.910          26.50            98.87   \n",
       "4           0.1809  ...        22.540          16.67           152.20   \n",
       "..             ...  ...           ...            ...              ...   \n",
       "564         0.1726  ...        25.450          26.40           166.10   \n",
       "565         0.1752  ...        23.690          38.25           155.00   \n",
       "566         0.1590  ...        18.980          34.12           126.70   \n",
       "567         0.2397  ...        25.740          39.42           184.60   \n",
       "568         0.1587  ...         9.456          30.37            59.16   \n",
       "\n",
       "     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0        2019.0           0.16220            0.66560           0.7119   \n",
       "1        1956.0           0.12380            0.18660           0.2416   \n",
       "2        1709.0           0.14440            0.42450           0.4504   \n",
       "3         567.7           0.20980            0.86630           0.6869   \n",
       "4        1575.0           0.13740            0.20500           0.4000   \n",
       "..          ...               ...                ...              ...   \n",
       "564      2027.0           0.14100            0.21130           0.4107   \n",
       "565      1731.0           0.11660            0.19220           0.3215   \n",
       "566      1124.0           0.11390            0.30940           0.3403   \n",
       "567      1821.0           0.16500            0.86810           0.9387   \n",
       "568       268.6           0.08996            0.06444           0.0000   \n",
       "\n",
       "     concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                  0.2654          0.4601                  0.11890  \n",
       "1                  0.1860          0.2750                  0.08902  \n",
       "2                  0.2430          0.3613                  0.08758  \n",
       "3                  0.2575          0.6638                  0.17300  \n",
       "4                  0.1625          0.2364                  0.07678  \n",
       "..                    ...             ...                      ...  \n",
       "564                0.2216          0.2060                  0.07115  \n",
       "565                0.1628          0.2572                  0.06637  \n",
       "566                0.1418          0.2218                  0.07820  \n",
       "567                0.2650          0.4087                  0.12400  \n",
       "568                0.0000          0.2871                  0.07039  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['diagnosis'] = data['diagnosis'].replace({'M': 1, 'B': -1})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Partition to training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can partition using 80% training data and 20% testing data. It is a commonly used ratio in machinel learning.\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.drop(['diagnosis'], axis = 1),\n",
    "                                                    data['diagnosis'],\n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state = 99)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the standardization to trainsform both training and test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test mean = \n",
      "radius_mean               -0.129482\n",
      "texture_mean              -0.035220\n",
      "perimeter_mean            -0.125513\n",
      "area_mean                 -0.110192\n",
      "smoothness_mean           -0.038945\n",
      "compactness_mean          -0.073125\n",
      "concavity_mean            -0.040630\n",
      "concave points_mean       -0.066406\n",
      "symmetry_mean             -0.059605\n",
      "fractal_dimension_mean     0.036604\n",
      "radius_se                 -0.033157\n",
      "texture_se                 0.050095\n",
      "perimeter_se              -0.043073\n",
      "area_se                   -0.064465\n",
      "smoothness_se              0.000264\n",
      "compactness_se             0.019330\n",
      "concavity_se               0.065696\n",
      "concave points_se          0.065752\n",
      "symmetry_se               -0.088019\n",
      "fractal_dimension_se       0.080401\n",
      "radius_worst              -0.110786\n",
      "texture_worst             -0.006319\n",
      "perimeter_worst           -0.111726\n",
      "area_worst                -0.096243\n",
      "smoothness_worst          -0.058650\n",
      "compactness_worst         -0.096465\n",
      "concavity_worst           -0.062763\n",
      "concave points_worst      -0.075300\n",
      "symmetry_worst            -0.108585\n",
      "fractal_dimension_worst   -0.048092\n",
      "dtype: float64\n",
      "test std = \n",
      "radius_mean                0.994074\n",
      "texture_mean               0.924808\n",
      "perimeter_mean             0.996385\n",
      "area_mean                  1.013168\n",
      "smoothness_mean            0.890423\n",
      "compactness_mean           0.987681\n",
      "concavity_mean             1.063123\n",
      "concave points_mean        1.015808\n",
      "symmetry_mean              1.001194\n",
      "fractal_dimension_mean     1.084790\n",
      "radius_se                  0.898680\n",
      "texture_se                 0.845615\n",
      "perimeter_se               0.873320\n",
      "area_se                    0.728517\n",
      "smoothness_se              0.981892\n",
      "compactness_se             1.192257\n",
      "concavity_se               1.521856\n",
      "concave points_se          1.176081\n",
      "symmetry_se                0.911439\n",
      "fractal_dimension_se       1.484738\n",
      "radius_worst               0.978016\n",
      "texture_worst              0.976994\n",
      "perimeter_worst            0.979680\n",
      "area_worst                 0.964625\n",
      "smoothness_worst           0.898541\n",
      "compactness_worst          0.913212\n",
      "concavity_worst            0.948816\n",
      "concave points_worst       0.932626\n",
      "symmetry_worst             0.965222\n",
      "fractal_dimension_worst    0.950148\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Standardization\n",
    "import numpy as np\n",
    "\n",
    "# calculate mu and sig using the training set\n",
    "d = x_train.shape[1]\n",
    "mu = np.mean(x_train, axis=0).to_numpy().reshape(1, d)\n",
    "sig = np.std(x_train, axis=0).to_numpy().reshape(1, d)\n",
    "\n",
    "# transform the training features\n",
    "x_train = (x_train - mu) / (sig + 1e-6)\n",
    "\n",
    "# transform the test features\n",
    "x_test = (x_test - mu) / (sig + 1e-6)\n",
    "\n",
    "print('test mean = ')\n",
    "print(np.mean(x_test, axis=0))\n",
    "\n",
    "print('test std = ')\n",
    "print(np.std(x_test, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.to_numpy()\n",
    "y_train = y_train.to_numpy().reshape(-1, 1)\n",
    "x_test = x_test.to_numpy()\n",
    "y_test = y_test.to_numpy().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  Logistic Regression Model\n",
    "\n",
    "The objective function is $Q (w; X, y) = \\frac{1}{n} \\sum_{i=1}^n \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $.\n",
    "\n",
    "When $\\lambda = 0$, the model is a regular logistric regression and when $\\lambda > 0$, it essentially becomes a regularized logistric regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the objective function value, or loss\n",
    "# Inputs:\n",
    "#     w: weight: d-by-1 vector\n",
    "#     x: data: n-by-d matrix\n",
    "#     y: label: n-by-1 vector\n",
    "#     lam: regularization parameter: scalar\n",
    "# Return:\n",
    "#     objective function value, or loss (scalar)\n",
    "def objective(w, x, y, lam):\n",
    "    a=x @ w\n",
    "    \n",
    "    log_loss=np.mean(np.log(1+np.exp(-y*a)))\n",
    "    \n",
    "    if lam > 0:\n",
    "        \n",
    "        loss_reg = (lam / 2) * np.sum(w**2)  # L2 regularization\n",
    "        loss = log_loss + loss_reg  # Total loss with regularization\n",
    "    else:\n",
    "        loss = log_loss  # No regularization when λ = 0\n",
    "   \n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Numerical optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient at $w$ for regularized logistic regression is  $g = - \\frac{1}{n} \\sum_{i=1}^n \\frac{y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the gradient\n",
    "# Inputs:\n",
    "#     w: weight: d-by-1 matrix\n",
    "#     x: data: n-by-d matrix\n",
    "#     y: label: n-by-1 matrix\n",
    "#     lam: regularization parameter: scalar\n",
    "# Return:\n",
    "#     g: gradient: d-by-1 matrix\n",
    "\n",
    "\n",
    "def gradient(w, x, y, lam):\n",
    "    n = x.shape[0]\n",
    "    a=x @ w\n",
    "    \n",
    "    b=1+np.exp(y*a)\n",
    "    coef=-y/b\n",
    "    \n",
    "    if lam > 0:\n",
    "        gradient = (1 / n) * x.T @ coef + lam * w  # Regularized gradient\n",
    "    else:\n",
    "        gradient = (1 / n) * x.T @ coef  # Non-regularized gradient\n",
    "        \n",
    "        \n",
    "    \n",
    "    return gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent for solving logistic regression\n",
    "# You will need to do iterative process (loops) to obtain optimal weights in this function\n",
    "\n",
    "# Inputs:\n",
    "#     x: data: n-by-d matrix\n",
    "#     y: label: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "#     learning_rate: scalar\n",
    "#     w: weights: d-by-1 vector, initialization of w\n",
    "#     max_epoch: integer, the maximal epochs\n",
    "# Return:\n",
    "#     w: weights: d-by-1 vector, the solution\n",
    "#     objvals: a record of each epoch's objective value\n",
    "\n",
    "def gradient_descent(x, y, lam, learning_rate, w, max_epoch=100):\n",
    "    \n",
    "    obj_value=[]\n",
    "    \n",
    "    for i in range(max_epoch):\n",
    "    \n",
    "        grad=gradient(w, x, y, lam)\n",
    "\n",
    "        w=w-(learning_rate*grad)\n",
    "    \n",
    "        objective_value=objective(w, x, y, lam)\n",
    "        obj_value.append(objective_value)\n",
    "\n",
    "    \n",
    "    return w,np.array(obj_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use gradient_descent function to obtain your optimal weights and a list of objective values over each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized weights (w):\n",
      "[[ 0.39089585]\n",
      " [ 0.34445796]\n",
      " [ 0.38868331]\n",
      " [ 0.39565873]\n",
      " [ 0.14779099]\n",
      " [ 0.14534079]\n",
      " [ 0.3071577 ]\n",
      " [ 0.40308206]\n",
      " [ 0.10577875]\n",
      " [-0.14747846]\n",
      " [ 0.35668305]\n",
      " [ 0.02099074]\n",
      " [ 0.31535348]\n",
      " [ 0.31876279]\n",
      " [ 0.03448131]\n",
      " [-0.08212439]\n",
      " [-0.07662004]\n",
      " [ 0.09767343]\n",
      " [-0.04704764]\n",
      " [-0.16645448]\n",
      " [ 0.45049593]\n",
      " [ 0.39391959]\n",
      " [ 0.44062472]\n",
      " [ 0.43594007]\n",
      " [ 0.31893295]\n",
      " [ 0.23327074]\n",
      " [ 0.281366  ]\n",
      " [ 0.41084566]\n",
      " [ 0.27574743]\n",
      " [ 0.14113369]]\n",
      "\n",
      "Objective values during training:\n",
      "[0.52148675 0.43398544 0.38046731 0.34372916 0.3166104  0.29557174\n",
      " 0.27864909 0.26465872 0.25284159 0.24268671 0.2338364  0.22603206\n",
      " 0.21908164 0.21283914 0.20719141 0.20204917 0.19734094 0.19300872\n",
      " 0.18900486 0.18528981 0.18183042 0.17859869 0.17557072 0.17272603\n",
      " 0.17004692 0.16751801 0.16512584 0.1628586  0.16070587 0.15865841\n",
      " 0.15670798 0.15484723 0.15306956 0.15136904 0.14974032 0.14817853\n",
      " 0.14667928 0.14523855 0.14385268 0.14251833 0.14123242 0.13999213\n",
      " 0.13879487 0.13763824 0.13652003 0.13543819 0.13439081 0.13337614\n",
      " 0.13239252 0.13143844 0.13051246 0.12961325 0.12873957 0.12789025\n",
      " 0.12706419 0.12626036 0.1254778  0.12471561 0.12397292 0.12324894\n",
      " 0.12254289 0.12185406 0.12118178 0.1205254  0.11988431 0.11925794\n",
      " 0.11864573 0.11804717 0.11746177 0.11688906 0.11632859 0.11577994\n",
      " 0.1152427  0.11471649 0.11420094 0.1136957  0.11320044 0.11271483\n",
      " 0.11223858 0.11177138 0.11131295 0.11086304 0.11042138 0.10998772\n",
      " 0.10956184 0.10914349 0.10873248 0.10832858 0.10793159 0.10754133\n",
      " 0.10715761 0.10678025 0.10640907 0.10604391 0.10568462 0.10533103\n",
      " 0.10498301 0.1046404  0.10430308 0.1039709 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Setting the regularization parameter (lam)\n",
    "lam = 0\n",
    "\n",
    "#Initializing the weights (w)\n",
    "d = x_train.shape[1]\n",
    "w = np.zeros((d, 1))\n",
    "learning_rate=0.1\n",
    "max_epoch=100\n",
    "\n",
    "w_optimized, obj_values = gradient_descent(x_train, y_train, lam, learning_rate, w, max_epoch)\n",
    "\n",
    "print(\"Optimized weights (w):\")\n",
    "print(w_optimized)\n",
    "\n",
    "print(\"\\nObjective values during training:\")\n",
    "print(obj_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train regularized logistric regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "\n",
    "# Setting the regularization parameter (lam)\n",
    "lam_1 = 0.01\n",
    "\n",
    "#Initializing the weights (w)\n",
    "d = x_train.shape[1]\n",
    "w = np.zeros((d, 1))\n",
    "learning_rate=0.1\n",
    "max_epoch=100\n",
    "w_optimized_1, obj_values = gradient_descent(x_train, y_train, lam, learning_rate, w, max_epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Stochastic gradient descent (SGD)\n",
    "\n",
    "Define new objective function $Q_i (w) = \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $. \n",
    "\n",
    "The stochastic gradient at $w$ is $g_i = \\frac{\\partial Q_i }{ \\partial w} = -\\frac{y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$.\n",
    "\n",
    "You may need to implement a new function to calculate the new objective function and gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the objective Q_i and the gradient of Q_i\n",
    "# Inputs:\n",
    "#     w: weights: d-by-1 matrix\n",
    "#     xi: data: 1-by-d matrix\n",
    "#     yi: label: scalar\n",
    "#     lam: scalar, the regularization parameter\n",
    "# Return:\n",
    "#     obj: scalar, the objective Q_i\n",
    "#     g: d-by-1 matrix, gradient of Q_i\n",
    "\n",
    "def stochastic_objective_gradient(w, xi, yi, lam):\n",
    "    a=xi @ w\n",
    "    log_loss=np.log(1+np.exp(-yi*a))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    c=xi @ w\n",
    "    \n",
    "    b=1+np.exp(yi*c)\n",
    "    coef=-yi/b\n",
    "    gradient =xi.T @ coef \n",
    "    \n",
    "    if lam > 0:\n",
    "        \n",
    "    # Applying regularization if lambda > 0\n",
    "        gradient += lam * w\n",
    "    \n",
    "    \n",
    " \n",
    "\n",
    "    \n",
    "    return log_loss,gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hints:\n",
    "1. In every epoch, randomly permute the $n$ samples.\n",
    "2. Each epoch has $n$ iterations. In every iteration, use 1 sample, and compute the gradient and objective using the ``stochastic_objective_gradient`` function. In the next iteration, use the next sample, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD for solving logistic regression\n",
    "# You will need to do iterative process (loops) to obtain optimal weights in this function\n",
    "\n",
    "# Inputs:\n",
    "#     x: data: n-by-d matrix\n",
    "#     y: label: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "#     learning_rate: scalar\n",
    "#     w: weights: d-by-1 matrix, initialization of w\n",
    "#     max_epoch: integer, the maximal epochs\n",
    "# Return:\n",
    "#     \n",
    "#     w: weights: d-by-1 matrix, the solution\n",
    "#     objvals: a record of each epoch's objective value\n",
    "#     Record one objective value per epoch (not per iteration)\n",
    "\n",
    "def sgd(x, y, lam, learning_rate, w, max_epoch=100):\n",
    "    n=len(x)\n",
    "   \n",
    "    objvals = []\n",
    "       \n",
    "    for epoch in range(max_epoch):\n",
    "        shuffle_order = np.random.permutation(n)\n",
    "        x_shuffled = x[shuffle_order]\n",
    "        y_shuffled = y[shuffle_order]\n",
    "        \n",
    "        loss=0\n",
    "        \n",
    "        for i in range(n):\n",
    "            xi=x_shuffled[i,:].reshape(1, -1)\n",
    "            yi=y_shuffled[i].reshape(1,1) \n",
    "            \n",
    "        \n",
    "            loss_i, gradient = stochastic_objective_gradient(w, xi, yi, lam)\n",
    "            w=w-(gradient*learning_rate)\n",
    "        \n",
    "        \n",
    "            loss=loss+loss_i\n",
    "        \n",
    "        \n",
    "        loss_avg=(loss)/n\n",
    "        \n",
    "        \n",
    "        if lam > 0:\n",
    "                \n",
    "            loss_reg = (lam / 2) * np.sum(w**2)  # L2 regularization term\n",
    "        else:\n",
    "            loss_reg = 0\n",
    "        \n",
    "        # Total loss including regularization\n",
    "        final_loss = loss_avg + loss_reg\n",
    "        \n",
    "        objvals.append(final_loss)\n",
    "        \n",
    "        \n",
    "    return w,np.array(objvals)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use sgd function to obtain your optimal weights and a list of objective values over each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.66118062],\n",
       "       [ 0.99752915],\n",
       "       [ 0.96674438],\n",
       "       [-0.17626648],\n",
       "       [ 0.72015059],\n",
       "       [-2.95134333],\n",
       "       [ 2.82902406],\n",
       "       [ 3.47321194],\n",
       "       [-1.45603947],\n",
       "       [-0.18511232],\n",
       "       [ 4.91914701],\n",
       "       [-1.08323588],\n",
       "       [ 1.01835941],\n",
       "       [ 2.60274967],\n",
       "       [ 0.19773526],\n",
       "       [-2.03968692],\n",
       "       [-0.76225641],\n",
       "       [ 3.50327762],\n",
       "       [-1.49048237],\n",
       "       [-2.91969834],\n",
       "       [ 1.81725123],\n",
       "       [ 2.50895868],\n",
       "       [ 1.50401904],\n",
       "       [ 0.38304402],\n",
       "       [ 0.62448892],\n",
       "       [-1.30975299],\n",
       "       [ 1.2997472 ],\n",
       "       [ 1.21431199],\n",
       "       [ 2.64274761],\n",
       "       [ 3.65600146]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train logistic regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "\n",
    "# Setting the regularization parameter (lam)\n",
    "lam = 0\n",
    "\n",
    "#Initializing the weights (w)\n",
    "d = x_train.shape[1]\n",
    "w = np.zeros((d, 1))\n",
    "learning_rate=0.1\n",
    "max_epoch=100\n",
    "\n",
    "w_optimized, obj_values = sgd(x_train, y_train, lam, learning_rate, w, max_epoch)\n",
    "w_optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized weights (w):\n",
      "[[ 0.45552502]\n",
      " [ 0.45757597]\n",
      " [ 0.45075975]\n",
      " [ 0.52792513]\n",
      " [ 0.13705159]\n",
      " [-0.15485504]\n",
      " [ 0.48678186]\n",
      " [ 0.62960307]\n",
      " [-0.07567251]\n",
      " [-0.23164586]\n",
      " [ 0.75714869]\n",
      " [-0.10988772]\n",
      " [ 0.56499576]\n",
      " [ 0.63148495]\n",
      " [ 0.25172423]\n",
      " [-0.52109774]\n",
      " [-0.30900769]\n",
      " [ 0.12271216]\n",
      " [-0.19983637]\n",
      " [-0.351838  ]\n",
      " [ 0.63885566]\n",
      " [ 0.6144375 ]\n",
      " [ 0.60750167]\n",
      " [ 0.68017148]\n",
      " [ 0.56695742]\n",
      " [ 0.13888984]\n",
      " [ 0.38435729]\n",
      " [ 0.59806542]\n",
      " [ 0.49759476]\n",
      " [ 0.35320082]]\n",
      "\n",
      "Objective values during training:\n",
      "[[[0.13855421]]\n",
      "\n",
      " [[0.12199766]]\n",
      "\n",
      " [[0.1102463 ]]\n",
      "\n",
      " [[0.11607986]]\n",
      "\n",
      " [[0.10992561]]\n",
      "\n",
      " [[0.11349178]]\n",
      "\n",
      " [[0.11136611]]\n",
      "\n",
      " [[0.11098986]]\n",
      "\n",
      " [[0.10826036]]\n",
      "\n",
      " [[0.11279892]]\n",
      "\n",
      " [[0.10806759]]\n",
      "\n",
      " [[0.11078597]]\n",
      "\n",
      " [[0.1134826 ]]\n",
      "\n",
      " [[0.11255417]]\n",
      "\n",
      " [[0.11326561]]\n",
      "\n",
      " [[0.11686248]]\n",
      "\n",
      " [[0.10966611]]\n",
      "\n",
      " [[0.11349181]]\n",
      "\n",
      " [[0.11230636]]\n",
      "\n",
      " [[0.10768245]]\n",
      "\n",
      " [[0.11365613]]\n",
      "\n",
      " [[0.11274629]]\n",
      "\n",
      " [[0.10854084]]\n",
      "\n",
      " [[0.10912796]]\n",
      "\n",
      " [[0.10627823]]\n",
      "\n",
      " [[0.11752474]]\n",
      "\n",
      " [[0.11077467]]\n",
      "\n",
      " [[0.11189242]]\n",
      "\n",
      " [[0.11422204]]\n",
      "\n",
      " [[0.11092393]]\n",
      "\n",
      " [[0.10615003]]\n",
      "\n",
      " [[0.108528  ]]\n",
      "\n",
      " [[0.114738  ]]\n",
      "\n",
      " [[0.10959672]]\n",
      "\n",
      " [[0.11100386]]\n",
      "\n",
      " [[0.11082626]]\n",
      "\n",
      " [[0.10923513]]\n",
      "\n",
      " [[0.11113969]]\n",
      "\n",
      " [[0.11081772]]\n",
      "\n",
      " [[0.10966144]]\n",
      "\n",
      " [[0.11468683]]\n",
      "\n",
      " [[0.11351406]]\n",
      "\n",
      " [[0.11373096]]\n",
      "\n",
      " [[0.10710068]]\n",
      "\n",
      " [[0.11391213]]\n",
      "\n",
      " [[0.11912808]]\n",
      "\n",
      " [[0.11401573]]\n",
      "\n",
      " [[0.10418814]]\n",
      "\n",
      " [[0.11067395]]\n",
      "\n",
      " [[0.10679523]]\n",
      "\n",
      " [[0.11286437]]\n",
      "\n",
      " [[0.11116337]]\n",
      "\n",
      " [[0.10662195]]\n",
      "\n",
      " [[0.10779041]]\n",
      "\n",
      " [[0.10959462]]\n",
      "\n",
      " [[0.10892686]]\n",
      "\n",
      " [[0.10952538]]\n",
      "\n",
      " [[0.11626846]]\n",
      "\n",
      " [[0.10767453]]\n",
      "\n",
      " [[0.10965141]]\n",
      "\n",
      " [[0.11156867]]\n",
      "\n",
      " [[0.10657329]]\n",
      "\n",
      " [[0.11761626]]\n",
      "\n",
      " [[0.11056081]]\n",
      "\n",
      " [[0.11287282]]\n",
      "\n",
      " [[0.10850165]]\n",
      "\n",
      " [[0.11368895]]\n",
      "\n",
      " [[0.10815565]]\n",
      "\n",
      " [[0.10653611]]\n",
      "\n",
      " [[0.10650949]]\n",
      "\n",
      " [[0.10960969]]\n",
      "\n",
      " [[0.11182407]]\n",
      "\n",
      " [[0.11554215]]\n",
      "\n",
      " [[0.11352241]]\n",
      "\n",
      " [[0.11645187]]\n",
      "\n",
      " [[0.10928756]]\n",
      "\n",
      " [[0.11074078]]\n",
      "\n",
      " [[0.1102087 ]]\n",
      "\n",
      " [[0.11755331]]\n",
      "\n",
      " [[0.11080261]]\n",
      "\n",
      " [[0.10614949]]\n",
      "\n",
      " [[0.1089075 ]]\n",
      "\n",
      " [[0.10416703]]\n",
      "\n",
      " [[0.11998402]]\n",
      "\n",
      " [[0.11222591]]\n",
      "\n",
      " [[0.11283953]]\n",
      "\n",
      " [[0.11783573]]\n",
      "\n",
      " [[0.10745913]]\n",
      "\n",
      " [[0.11420438]]\n",
      "\n",
      " [[0.11073561]]\n",
      "\n",
      " [[0.1235283 ]]\n",
      "\n",
      " [[0.1096688 ]]\n",
      "\n",
      " [[0.10401618]]\n",
      "\n",
      " [[0.1103766 ]]\n",
      "\n",
      " [[0.11449636]]\n",
      "\n",
      " [[0.1159769 ]]\n",
      "\n",
      " [[0.11260178]]\n",
      "\n",
      " [[0.11349557]]\n",
      "\n",
      " [[0.11120036]]\n",
      "\n",
      " [[0.1108427 ]]]\n"
     ]
    }
   ],
   "source": [
    "# Train regularized logistic regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "\n",
    "lam = 0.01\n",
    "\n",
    "#Initializing the weights (w)\n",
    "d = x_train.shape[1]\n",
    "w = np.zeros((d, 1))\n",
    "learning_rate=0.1\n",
    "max_epoch=100\n",
    "\n",
    "w_optimized_2, obj_values_2 = sgd(x_train, y_train, lam, learning_rate, w, max_epoch)\n",
    "print(\"Optimized weights (w):\")\n",
    "print(w_optimized_2)\n",
    "\n",
    "print(\"\\nObjective values during training:\")\n",
    "print(obj_values_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Mini-Batch Gradient Descent (MBGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define $Q_I (w) = \\frac{1}{b} \\sum_{i \\in I} \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $, where $I$ is a set containing $b$ indices randomly drawn from $\\{ 1, \\cdots , n \\}$ without replacement.\n",
    "\n",
    "The stochastic gradient at $w$ is $g_I = \\frac{\\partial Q_I }{ \\partial w} = \\frac{1}{b} \\sum_{i \\in I} \\frac{- y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$.\n",
    "\n",
    "You may need to implement a new function to calculate the new objective function and gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the objective Q_I and the gradient of Q_I\n",
    "# Inputs:\n",
    "#     w: weights: d-by-b matrix\n",
    "#     xi: data: b-by-d matrix\n",
    "#     yi: label: scalar\n",
    "#     lam: scalar, the regularization parameter\n",
    "# Return:\n",
    "#     obj: scalar, the objective Q_i\n",
    "#     g: d-by-1 matrix, gradient of Q_i\n",
    "\n",
    "def mb_objective_gradient(w, xi, yi, lam):\n",
    "    \n",
    "    a=xi @ w\n",
    "    log_loss=np.mean(np.log(1+np.exp(-yi*a)))\n",
    "  \n",
    "    \n",
    "    \n",
    "    c=xi @ w\n",
    "    \n",
    "    b=1+np.exp(yi*c)\n",
    "    coef=-yi/b\n",
    "    gradient =xi.T @ coef/xi.shape[0]\n",
    "    if lam > 0:\n",
    "        \n",
    "    # Applying regularization if lambda > 0\n",
    "        gradient += lam * w\n",
    "    \n",
    "\n",
    "    \n",
    "    return log_loss,gradient\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hints:\n",
    "1. In every epoch, randomly permute the $n$ samples (just like SGD).\n",
    "2. Each epoch has $\\frac{n}{b}$ iterations. In every iteration, use $b$ samples, and compute the gradient and objective using the ``mb_objective_gradient`` function. In the next iteration, use the next $b$ samples, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MBGD for solving logistic regression\n",
    "# You will need to do iterative process (loops) to obtain optimal weights in this function\n",
    "\n",
    "# Inputs:\n",
    "#     x: data: n-by-d matrix\n",
    "#     y: label: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "#     learning_rate: scalar\n",
    "#     w: weights: d-by-1 matrix, initialization of w\n",
    "#     max_epoch: integer, the maximal epochs\n",
    "# Return:\n",
    "#     w: weights: d-by-1 matrix, the solution\n",
    "#     objvals: a record of each epoch's objective value\n",
    "#     Record one objective value per epoch (not per iteration)\n",
    "\n",
    "def mbgd(x, y, lam, learning_rate, w, max_epoch=100, batch_size=32):\n",
    "    \n",
    "    n=len(x)\n",
    "   \n",
    "    objvals = []\n",
    "       \n",
    "    for epoch in range(max_epoch):\n",
    "        shuffle_order = np.random.permutation(n)\n",
    "        x_shuffled = x[shuffle_order]\n",
    "        y_shuffled = y[shuffle_order].reshape(-1, 1)\n",
    "        num_batches = max(1, n // batch_size)  \n",
    "\n",
    "        \n",
    "        loss=0\n",
    "        \n",
    "        for i in range(0,n,batch_size):\n",
    "            xi=x_shuffled[i:i+batch_size,:]\n",
    "            yi=y_shuffled[i:i+batch_size]\n",
    "            \n",
    "        \n",
    "            loss_i, gradient = mb_objective_gradient(w, xi, yi, lam)\n",
    "            w=w-(gradient*learning_rate)\n",
    "        \n",
    "        \n",
    "            loss=loss+loss_i\n",
    "        \n",
    "        \n",
    "        loss_avg=(loss)/num_batches\n",
    "        \n",
    "        \n",
    "        if lam > 0:\n",
    "            #  regularization term if lambda > 0\n",
    "            loss_reg = (lam / 2) * np.sum(w**2)\n",
    "        else:\n",
    "            loss_reg = 0  # No regularization if lambda = 0\n",
    "        \n",
    "\n",
    "        final_loss=loss_avg+loss_reg\n",
    "        \n",
    "        objvals.append(final_loss)\n",
    "        \n",
    "        \n",
    "    return w,np.array(objvals)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.53163455],\n",
       "       [ 0.74250211],\n",
       "       [ 0.52578289],\n",
       "       [ 0.66253629],\n",
       "       [ 0.21926582],\n",
       "       [-0.27376666],\n",
       "       [ 0.72699605],\n",
       "       [ 0.88207792],\n",
       "       [-0.08232525],\n",
       "       [-0.24806748],\n",
       "       [ 1.15303637],\n",
       "       [-0.05697736],\n",
       "       [ 0.85624593],\n",
       "       [ 0.92083973],\n",
       "       [ 0.27623445],\n",
       "       [-0.80850835],\n",
       "       [-0.16506893],\n",
       "       [ 0.27154234],\n",
       "       [-0.16317231],\n",
       "       [-0.56310942],\n",
       "       [ 0.8591483 ],\n",
       "       [ 0.92108266],\n",
       "       [ 0.80206176],\n",
       "       [ 0.94374012],\n",
       "       [ 0.90890178],\n",
       "       [ 0.13715654],\n",
       "       [ 0.6675489 ],\n",
       "       [ 0.88649898],\n",
       "       [ 0.72298931],\n",
       "       [ 0.52598455]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lam = 0\n",
    "\n",
    "#Initializing the weights (w)\n",
    "d = x_train.shape[1]\n",
    "w = np.zeros((d, 1))\n",
    "learning_rate=0.1\n",
    "max_epoch=100\n",
    "\n",
    "w_optimized, obj_values = mbgd(x_train, y_train, lam, learning_rate, w, max_epoch)\n",
    "w_optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use mbgd function to obtain your optimal weights and a list of objective values over each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.53163455],\n",
       "       [ 0.74250211],\n",
       "       [ 0.52578289],\n",
       "       [ 0.66253629],\n",
       "       [ 0.21926582],\n",
       "       [-0.27376666],\n",
       "       [ 0.72699605],\n",
       "       [ 0.88207792],\n",
       "       [-0.08232525],\n",
       "       [-0.24806748],\n",
       "       [ 1.15303637],\n",
       "       [-0.05697736],\n",
       "       [ 0.85624593],\n",
       "       [ 0.92083973],\n",
       "       [ 0.27623445],\n",
       "       [-0.80850835],\n",
       "       [-0.16506893],\n",
       "       [ 0.27154234],\n",
       "       [-0.16317231],\n",
       "       [-0.56310942],\n",
       "       [ 0.8591483 ],\n",
       "       [ 0.92108266],\n",
       "       [ 0.80206176],\n",
       "       [ 0.94374012],\n",
       "       [ 0.90890178],\n",
       "       [ 0.13715654],\n",
       "       [ 0.6675489 ],\n",
       "       [ 0.88649898],\n",
       "       [ 0.72298931],\n",
       "       [ 0.52598455]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train regularized logistric regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "lam = 0.01\n",
    "\n",
    "#Initializing the weights (w)\n",
    "d = x_train.shape[1]\n",
    "w = np.zeros((d, 1))\n",
    "learning_rate=0.1\n",
    "max_epoch=100\n",
    "\n",
    "w_optimized_3, obj_values_3 = mbgd(x_train, y_train, lam, learning_rate, w, max_epoch)\n",
    "w_optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Compare GD, SGD, MBGD\n",
    "\n",
    "### Plot objective function values against epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_optimized, obj_values_gd = gradient_descent(x_train, y_train, lam, learning_rate, w, max_epoch)\n",
    "w_optimized, obj_values_sgd = sgd(x_train, y_train, lam, learning_rate, w, max_epoch)\n",
    "w_optimized, obj_values_mbgd = mbgd(x_train, y_train, lam, learning_rate, w, max_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_values_gd = obj_values_gd.flatten()\n",
    "obj_values_sgd = obj_values_sgd.flatten()\n",
    "obj_values_mbgd = obj_values_mbgd.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAGDCAYAAABjkcdfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/GU6VOAAAACXBIWXMAAAsTAAALEwEAmpwYAABsWUlEQVR4nO3dd3gUVdvH8e9JIyGB0ALSpIP0qgg27NhQUR8Ve39V7PpYUcTy2DsW7F2xI1YssVEUEKmi0qT3mgBp9/vH2SUhpGwgm92E3+e65trdmdmZe/fs7tx7zpkzzswQERERkYoVE+kARERERHZHSsJEREREIkBJmIiIiEgEKAkTERERiQAlYSIiIiIRoCRMREREJAKUhIlIsZxzZzjnvo50HEHOuSTn3KfOufXOufciHU9V4pw71zn3c6TjENmdKAkTqQDOuUHOuYnOuU3OuaXOuS+cc/tHOq7SmNmbZnZEpOMo4GSgAVDXzE4pagXnXBvn3DvOuZXOuQ3Oub+dc08655oElvdzzuUFymKTc26Rc26kc27vUINwziU45x4OPHeTc26+c+6xQuuc5pyb4JzLcM6tCNy/zDnnAstfcc5lOec2Bqbpzrn/OedSd/rdCRPnXHPnnDnnfi80v17gNcwvMG++c25z4H1Z65z7zDnXtNDzqsx7I7IrlISJhJlz7lrgMeBefAKxJ/A0cHwEwyqVcy4u0jEUoRnwl5nlFLXQOdcamAAsAbqbWU1gP2AOUDDpXWJmKUANYF/gT+An59yhIcZxM9AL2CewjX7A5AJxXAc8DjwI7IEv9/8LxJJQYDsPmFkNIA04LxDLL8655BDjqGjVnXOdCjweBMwrYr3jAu9vQ2A58GRwQRV+b0TKzsw0adIUpglIBTYBp5SwTjV8krYkMD0GVAss6wcsAv4LrACWAicARwN/AWuAWwpsayjwPvAusBGfGHQtsPwmfEKyEZgJnFhg2bnAL8CjwGrg7sC8nwPLXWDZCmADMA3oVOB1vgasBBYAtwExBbb7M/AQsBZ/0D6qhPejPZAOrANmAAMC8+8EsoDswHt6QRHPfQP4tJQy6QcsKmL+U8DEEMt1NHB1CWWeAZxUyjZeAe4uNK9GoIwHhxjHecCsQHnOBS4p/DqB6wp8ds4rsLwuMCpQlr8CdwXLuoj9NAcsUK4PFpg/EbgVmF9g3nzgsAKPj8YnzhX63mjSVBkm1YSJhFcfIBH4qIR1bsX/y+8GdMXXrtxWYPkegW00Bm4HngfOBHoCBwBDnHMtCqx/PPAeUAd4C/jYORcfWDYn8JxUfFLzhnOuYYHn9sYfzBsA9xSK8wjgQKBt4Pn/wSdr4Gs6UoGWwEHA2fgEoeB2ZwP1gAeAF4NNTwUF4vwU+BqoD1wBvOmca2dmd+BrE981sxQze7Hw84HDgA+KmB+KD4EeIda0jAeuDTShdS70WvrgE+tPyhqAmW0ExuDLKBQrgGOBmvj3+1HnXI8Cy/fAl0tj4AJguHOudmDZcGALvrbq/MBUmjeA05xzsc65DkAKvuaxSM656sCp+PcLKva9EYl6SsJEwqsusMqKaT4LOAMYZmYrzGwlPjk6q8DybOAeM8sG3sEnMo+b2UYzm4Gv0epaYP1JZvZ+YP1H8AncvgBm9p6ZLTGzPDN7F/gbn/QFLTGzJ80sx8w2F4ozG18bsRfgzGyWmS11zsUCpwE3B2KaDzxc6DUsMLPnzSwXeBV/4G9QxHuxL/7Afp+ZZZnZd/hap9NLeP8KqgcsCz5wzg12zq0L9E96vpTnLsHX9tUKYT//A+7Hl91EYLFz7pwCMWxX5s65sYE4NjvnDgwhjjohxICZfWZmc8z7AZ+8FkxSsvGfrWwz+xxfg9guUGYnAbebWYaZTceXS2kW4ZPpw/CJ9uvFrPexc24dsB44HN/0CBX43ohUBkrCRMJrNVCvlP5VjfBNeEELAvO2bSOQvAAEE6PlBZZvxicuQQuDd8wsD3/gbATgnDvbOTclcNBbB3TCHxh3eG5hgYToKXwNygrn3AjnXM3A8+OLeA2NCzxeVmA7mYG7BWMOagQsDMRd3LZKshqf4AX39ZSZ1cI38cYX85ygxvgmt3Wl7cTMcs1suJnth0/a7gFecs61p4gyN7O+gThWU/rvbmN8M3OpnHNHOefGO+fWBMrzaLYvz9WF/gBk4t/3NCCO7cu7YPmV5DV8E/PpFJ+EnRB4vYnAYOAH59weVOB7I1IZKAkTCa9xwFZ8P67iLMF3OA/aMzBvZ207E805FwM0AZY455rhmzIH488urAVMx9f+BFlJGzazJ8ysJ9AB3yx5A7AKX+NS+DUs3onYlwBNA3HvzLa+BQbuxH4BTgQmm1lGWZ5kZpvNbDi+v1sH8su8zCdeOOdS8LVMP4WwbjV80+tDQINAeX7O9uVZnJVADgU+K/j3ORQfAMcAc83s35JWDCSrHwK5+BMjKuS9EakslISJhJGZrcf34xrunDvBOVfdORcfqMF4ILDa28Btzrk051y9wPpv7MJuezrnBgZqG67GH/TGA8n4JGslgHPuPHxNWEicc3s753oH+m1l4PsT5QVq6UYC9zjnagSSvWt38jVMwNfW/DfwPvUDjsM3w4ZiKHCAc+4R51zjQNz18J39i3pNzjnX2Dl3B3AhcEuBZenOuaHFPO9q54e6SHLOxQWaImsAv5vZOnyT8tPOuZMD70mMc64bvgyK2l4151xP4GN8MvdyYH5waIjmRTwtAd+/aiWQ45w7Ct9vr1SBMvsQGBr4THYAzinlacHnZgCH4N+vEgXe3+OB2sCs8nxvRKqCaDwFXaRKMbOHnXPL8J3t38SfyTaJ/I7vd+M7Vk8NPH4vMG9nfYLvDP0q8A8wMNA/bKZz7mF8bUQevlnplzJstyb+7MiW+ATsK/L7+lyB75w/N7DseeClsgZuZlnOuePwQ3jcjK8BO9vM/gzx+X8553rjz/T7I1BbtATfV+qBAqs2cs5twtcarQfGAv3MbHyBdZpS/PuTie/31hqf2P6FP+NvbiCOB5xzi/Fntb6GT1rnAjcG9hX0X+fcVYE4FuD7v51coDauaWD+DjWBZrbROXclPgGuhj+hYVTJ79B2BuMTmmX4ITpeBg4O5YlmNrGUVT51zuXi35sFwDmB/ovl+d6IVHrOrMTWBxGpRAI1N63N7MxIx1KZOT+w60gz6xvhOG4DVprZc5GMQ0TCQzVhIiKFmNkiIKIJWCCOXakRFZEopz5hIiIiIhGg5kgRERGRCFBNmIiIiEgEKAkTERERiYBK1zG/Xr161rx587DuIyMjg+TkUC4fJxVNZROdVC7RS2UTnVQu0au8y2bSpEmrzCytqGWVLglr3rw5EyeWNkTNrklPT6dfv35h3YfsHJVNdFK5RC+VTXRSuUSv8i4b51yxlwRTc6SIiIhIBCgJExEREYkAJWEiIiIiEVDp+oSJiIhI9MjOzmbRokVs2bIl0qGUi9TUVGbNmlXm5yUmJtKkSRPi4+NDfo6SMBEREdlpixYtokaNGjRv3hznXKTD2WUbN26kRo0aZXqOmbF69WoWLVpEixYtQn6emiNFRERkp23ZsoW6detWiQRsZznnqFu3bplrA5WEiYiIyC7ZnROwoJ15D5SEiYiISKW3fPlyBg0aRMuWLenZsyd9+vTho48+Ij09ndTUVLp37067du048MADGT16dKTDBdQnTERERCo5M+OEE07gnHPO4a233gJgwYIFjBo1itq1a3PAAQdsS7ymTJnCCSecQFJSEoceemgkw1ZNmIiIiFRu3333HQkJCfzf//3ftnnNmjXjiiuu2GHdbt26cfvtt/PUU09VZIhFUk2YiIiIlIurr4YpU8p3m926wWOPlbzOjBkz6NGjR8jb7NGjBw8++OAuxVUeVBNWyIoVMG5cHTZujHQkIiIisjMuv/xyunbtyt57713kcjOr4IiKppqwQn75BW65pQtHHgllSKpFRER2e6XVWIVLx44d+eCDD7Y9Hj58OKtWraJXr15Frv/777/Tvn37igqvWKoJK6R+fX+7cmVk4xAREZHQHHLIIWzZsoVnnnlm27zMzMwi1506dSp33XUXl19+eUWFVyzVhBWSluZvlYSJiIhUDs45Pv74Y6655hoeeOAB0tLSSE5O5v777wfgp59+onv37mRmZlK/fn2eeOKJiJ8ZCUrCdhBMwlasiGwcIiIiErqGDRvyzjvvFLls/fr1FRxNaNQcWUitWhAbm6eaMBEREQkrJWGFOAe1amUrCRMREZGwUhJWhNRUJWEiIiISXkrCilCrVrb6hImIiEhYKQkrQq1aWaoJExERkbBSElYE9QkTERGRcFMSVoRatbLZsAG2bo10JCIiIhKKe+65h44dO9KlSxe6devGhAkTyMnJ4ZZbbqFNmzZ069aNbt26cc8992x7TmxsLN26daNjx4507dqVhx9+mLy8vAqLWeOEFSE1NQvwA7Y2aRLhYERERKRE48aNY/To0UyePJlq1aqxatUqsrKyuO2221i2bBnTpk0jMTGRjRs38vDDD297XlJSElMCVxxfsWIFgwYNYuXKldx3330VEndYa8Kcc/2dc7Odc/84524qYvm5zrmVzrkpgenCcMYTqtq1swGNmi8iIlIZLF26lHr16lGtWjUA6tWrR61atXj++ed58sknSUxMBKBGjRoMHTq0yG3Ur1+fESNGMGLEiAq7wHfYasKcc7HAcOBwYBHwm3NulJnNLLTqu2Y2OFxx7IxatZSEiYiIlNnVV0OgZqncdOtW6pXBjzjiCIYNG0bbtm057LDDOPXUU6lduzZ77rknNWrUCHlXLVu2JC8vjxUrVtCgQYNdizsE4awJ2wf4x8zmmlkW8A5wfBj3V26CzZEapkJERCT6paSkMGnSJEaMGEFaWhqnnnoq6enp263z8ssv061bN5o2bcrChQsjE2gh4ewT1hgo+CoXAb2LWO8k59yBwF/ANWYW8XdGzZEiIiI7oZQaq3CKjY2lX79+9OvXj86dO/Pcc8/x77//snHjRmrUqMF5553HeeedR6dOncjNzS1yG3PnziUmJob69etXSMyR7pj/KfC2mW11zl0CvAocUngl59zFwMUADRo02CG7LX+biI3NY+LEhaSnzwvzvqQsNm3aVAHlL2WlcoleKpvoVJXKJTU1lY0bN0Y0hr///hvnHK1btwZgwoQJtGjRgo4dO3LJJZfw+OOPk5iYSG5uLlu2bGHTpk3bYg7erlq1igsvvJCLLrqITZs27VQcW7ZsKVO5hjMJWww0LfC4SWDeNma2usDDF4AHitqQmY0ARgD06tXL+vXrV66BFpaenk5aWgzVqzejX79mYd2XlE16ejrhLn8pO5VL9FLZRKeqVC6zZs0qU7+rcDAzBg8ezLp164iLi6N169aMGDGC1NRUhgwZQp8+fahRowZJSUmcd955tG3bloSEBDZv3swBBxxAdnY2cXFxnHXWWVx00UU7/XoSExPp3r17yOuHMwn7DWjjnGuBT75OAwYVXME519DMlgYeDgBmhTGeMklLU58wERGRyqBnz56MHTu2yGX33XdfsUNOFNUsWZG1emFLwswsxzk3GPgKiAVeMrMZzrlhwEQzGwVc6ZwbAOQAa4BzwxVPWdWvrz5hIiIiEj5h7RNmZp8Dnxead3uB+zcDN4czhp2VlgYTJ0Y6ChEREamqdNmiYqg5UkRERMJJSVgx6tdH148UERGRsFESVoy0NH+7alVk4xAREZGqSUlYMYJJmDrni4iISDgoCStGMAlTvzAREZHo5pzjzDPP3PY4JyeHtLQ0jj32WABeeeUV0tLS6NatGx07duTkk08mMzNz2/qPPPIIe+21F507d6Zv375ce+21ZGf7q+c0b96czp0707lzZzp06MBtt93Gli1byiVuJWHFCF6xQDVhIiIi0S05OZnp06ezefNmAMaMGUPjxo23W+fUU09lypQpzJgxg4SEBN59910Ann32Wb7++mvGjx/PtGnTSE9Pp379+tu2BfD9998zbdo0fv31V+bOncsll1xSLnErCSuGmiNFREQqj6OPPprPPvsMgLfffpvTTz+9yPVycnLIyMigdu3aANxzzz0888wz1KpVC4CEhARuuukmatasucNzU1JSePbZZ/n4449Zs2bNLscc6WtHRq1atSA2Vs2RIiIiobr6y6uZsmxKuW6z2x7deKz/Y6Wud9pppzFs2DCOPfZYpk6dyvnnn89PP/20bfm7777Lzz//zNKlS2nbti3HHXccGzZsYNOmTbRo0SLkeGrWrEmLFi34+++/6d279868pG1UE1aMmBhfG6aaMBERkejXpUsX5s+fz9tvv83RRx+9w/Jgc+SyZcvo3LkzDz744A7rfPXVV+y33340b9682Msggb9WZXlQTVgJlISJiIiELpQaq3AaMGAA119/Penp6axevbrIdZxzHHfccTz55JPcdNNNpKSkMG/ePFq0aMGRRx5J3759Of3008nKyiry+Rs3bmT+/Pm0bdt2l+NVTVgJlISJiIhUHueffz533HEHnTt3LnG9n3/+mVatWgFw8803c+mll7Ju3TrA13IVd/bjpk2buOyyyzjhhBO29SnbFaoJK0FaGkyaFOkoREREJBRNmjThyiuvLHJZsE9YXl4eTZo04ZVXXgHg0ksvJSMjg969e1OtWjWSkpI48MAD6d69+7bnHnzwwZgZeXl5nHjiiQwZMqRc4lUSVoL69VUTJiIiEu02bdq0w7x+/frRr18/AM4991zOPffcIp/rnOOGG27ghhtuAHxzY40aNbYtnz9/fnmHu42aI0uQlgbr10MxzcIiIiIiO01JWAl0/UgREREJFyVhJdCli0RERCRclISVQJcuEhERKV15jZtVme3Me6AkrAS6dJGIiEjJEhMTWb169W6diJkZq1evJjExsUzP09mRJVBzpIiISMmaNGnCokWLWFlFaiy2bNlS5mQKfDLapEmTMj1HSVgJatf214+sIp8rERGRchcfH1+may9Gu/T09O3GCAsnNUeWICYG6tVTEiYiIiLlT0lYKXTpIhEREQkHJWGlSEtTnzAREREpf0rCSqFLF4mIiEg4KAkrhZojRUREJByUhJUiLQ3WrdP1I0VERKR8KQkrRXDUfF0/UkRERMqTkrBSaNR8ERERCQclYaVQEiYiIiLhoCSsFLp0kYiIiISDkrBSBPuEqSZMREREypOSsFLo+pEiIiISDkrCShETA3XrKgkTERGR8qUkLAS6dJGIiIiUNyVhIdCli0RERKS8KQkLgS5dJCIiIuVNSVgIlISJiIhIeVMSFoL69WHtWsjOjnQkIiIiUlUoCQtBcMBWXT9SREREyouSsBDo0kUiIiJS3pSEhUCXLhIREZHypiQsBI0a+dvFiyMbh4iIiFQdSsJC0KyZHzl/zpxIRyIiIiJVhZKwECQkwJ57KgkTERGR8qMkLEQtWyoJExERkfKjJCxErVopCRMREZHyoyQsRK1a+XHCNmyIdCQiIiJSFSgJC1GrVv5WtWEiIiJSHpSEhSiYhM2dG9k4REREpGpQEhYi1YSJiIhIeVISFqKaNaFePSVhIiIiUj6UhJWBzpAUERGR8qIkrAw0VpiIiIiUFyVhZdCqFfz7L2RlRToSERERqeyUhJVBq1aQlwcLFkQ6EhEREanslISVgc6QFBERkfKiJKwMNFaYiIiIlBclYWXQsCEkJakmTERERHadkrAycE5nSIqIiEj5UBJWRhorTERERMqDkrAyatXK9wkzi3QkIiIiUpkpCSujli0hMxOWLYt0JCIiIlKZhTUJc871d87Nds7945y7qYT1TnLOmXOuVzjjKQ8apkJERETKQ9iSMOdcLDAcOAroAJzunOtQxHo1gKuACeGKpTwpCRMREZHyEM6asH2Af8xsrpllAe8Axxex3l3A/cCWMMZSbpo3h5gYJWEiIiKya+LCuO3GwMICjxcBvQuu4JzrATQ1s8+cczcUtyHn3MXAxQANGjQgPT29/KMtYNOmTSXuIy1tX8aNW096+qywxiE7Kq1sJDJULtFLZROdVC7RqyLLJpxJWImcczHAI8C5pa1rZiOAEQC9evWyfv36hTW29PR0StpHx46waVMi/fo1CGscsqPSykYiQ+USvVQ20UnlEr0qsmxCao50ziU559qVcduLgaYFHjcJzAuqAXQC0p1z84F9gVGVpXO+miNFRERkV5SahDnnjgOmAF8GHndzzo0KYdu/AW2ccy2ccwnAacC255nZejOrZ2bNzaw5MB4YYGYTy/4yKlarVrByJWzcGOlIREREpLIKpSZsKL6T/ToAM5sCtCjtSWaWAwwGvgJmASPNbIZzbphzbsBOxhsVWrb0t6oNExERkZ0VSp+wbDNb75wrOC+k8eLN7HPg80Lzbi9m3X6hbDMaFBymolu3iIYiIiIilVQoSdgM59wgINY51wa4Ehgb3rCim8YKExERkV0VSnPkFUBHYCvwNrABuDqMMUW91FSoW1dJmIiIiOy8UmvCzCwTuDUwSUDwQt4iIiIiO6PUJMw59z1F9AEzs0PCElEl0aoVjB8f6ShERESksgqlT9j1Be4nAicBOeEJp/Jo1QpGjoTsbIiPj3Q0IiIiUtmE0hw5qdCsX5xzv4YpnkqjVSvIzYUFC6B160hHIyIiIpVNKM2RdQo8jAF6Aqlhi6iSKDhWmJIwERERKatQmiMn4fuEOXwz5DzggnAGVRkEE69//oEjj4xsLCIiIlL5hNIcWero+Lujhg2hTh2YMiXSkYiIiEhlVGwS5pwbWNITzezD8g+n8nAOevaESYV7zImIiIiEoKSasONKWGbAbp2EgU/CHn4Ytm6FatUiHY2IiIhUJsUmYWZ2XkUGUhn17OmHqJg2DXr1inQ0IiIiUpmE0jEf59wx+EsXJQbnmdmwcAVVWfTs6W8nTVISJiIiImVT6rUjnXPPAqfiryHpgFOAZmGOq1Jo3hxq11a/MBERESm7UC7g3dfMzgbWmtmdQB+gbXjDqhzUOV9ERER2VihJ2ObAbaZzrhGQDTQMX0iVS48evk/Y1q2RjkREREQqk1CSsNHOuVrAg8BkYD7wVhhjqlSCnfOnT490JCIiIlKZlDRO2Of4ZOtRM9sEfOCcGw0kmtn6igow2hXsnB+8LyIiIlKakmrCngOOAeY650Y6504ETAnY9lq2hFq11C9MREREyqbYJMzMPjGz04HmwAfA2cC/zrmXnXOHV1B8Uc853y9MSZiIiIiURal9wsws08zeNbMTgSOAbsCX4Q4sUn7+92eu++M65q6dG/Jzevb0nfOzssIYmIiIiFQpoYwT1sA5d4Vz7hfgY+AroEe4A4uU9VvWM3ndZFZlrgr5OT17+gRMnfNFREQkVMUmYc65i5xz3+HPiGwD3GBmLc3sJjP7o8IirGDJCckAZGRlhPycgp3zRUREREJRUk1YH+B/QFMzu9LMxlZQTBGVHO+TsE1Zm0J+TqtWkJqqJExERERCV9IFvM+vyECixbaasOzQa8LUOV9ERETKKpTBWncrKQkpQNmaI8E3SU6dqs75IiIiEholYYUEmyPLUhMG+Z3zZ8wIR1QiIiJS1YSUhDnnYp1zjZxzewancAcWKcHmyLL0CQN1zhcREZGyCWWIiiuA5cAY4LPANDrMcUVMtdhqxBBT5ubIVq2gZk0lYSIiIhKaYjvmF3AV0M7MVoc7mGjgnCMpNqnMzZExMeqcLyIiIqELpTlyIbBbXS8yMTaxzDVhkN85Pzs7DEGJiIhIlRJKTdhcIN059xmwNTjTzB4JW1QRlhibyKbssvUJA5+Ebd3qO+d361b+cYmIiEjVEUpN2L/4/mAJQI0CU5WVGLPzNWGgJkkREREpXak1YWZ2J4BzLiXwuOxVRJXMzvQJA2jd2nfOHz8eLrggDIGJiIhIlRHK2ZGdnHO/AzOAGc65Sc65juEPLXJ2tk9YTAz06wfffANm5R+XiIiIVB2hNEeOAK41s2Zm1gy4Dng+vGFFVlJsUpnHCQs64giYPx/mzCnfmERERKRqCSUJSzaz74MPzCwdSA5bRFEgMSZxp5ojAQ4/3N+OGVOOAYmIiEiVE0oSNtc5N8Q51zww3YY/Y7LKSopN2qnmSIA2bWDPPeHrr8s5KBEREalSQknCzgfSgA8DU1pgXpWVGLvzNWHO+SbJ776DnJxyDkxERESqjFKTMDNba2ZXmlmPwHSVma2tiOAiJTE2kczsTPIsb6eef/jhsGED/PZbOQcmIiIiVUaxQ1Q45x4zs6udc58CO5zrZ2YDwhpZBCXGJAKQmZ1JSkJKmZ9/6KG+RmzMGOjTp7yjExERkaqgpHHCXg/cPlQRgUSTpNgkADKyMnYqCatb119HcswYuP328o5OREREqoJimyPNLDjuezcz+6HgBHSrkOgiJDHW14TtbL8w8P3Cxo3zzZIiIiIihYXSMf+cIuadW85xRJVgTdjOjhUGvl9Ybi6kp5dTUCIiIlKllNQn7HRgENDCOTeqwKIawJpwBxZJBZsjd1bfvlC9um+SHFBle8+JiIjIziqpT9hYYClQD3i4wPyNwNRwBhVpwY75u9IcWa0aHHigBm0VERGRopXUJ2xBYHT8M4AJBfqDzQKaVFB8EbGtT9gu1ISB7xc2ezb8+295RCUiIiJVSSh9wkYCBQfMygXeC0840SGYhO1KnzDQJYxERESkeKEkYXFmlhV8ELifEL6QIm9bn7BdaI4E6NgRGjZUEiYiIiI7CiUJW+mc29a13Dl3PLAqfCFFXnl0zAc/YOthh8G330Lezg2+LyIiIlVUKEnY/wG3OOf+dc4tBG4ELglvWJFVHh3zg444AlatgilTdnlTIiIiUoWUdHYkAGY2B9jXOZcSeLxrHaUqgbiYOOJj4ne5Txj4mjCAr7/2o+iLiIiIQAhJmHOuGnAS0ByIc84BYGbDwhpZhKUkpOxycyTAHntA164wejTcdFM5BCYiIiJVQijNkZ8AxwM5QEaBqUpLTkgul+ZIgFNOgV9+0VAVIiIikq/UmjCgiZn1D3skUSY5vvySsNNPh9tug3fegf/+t1w2KSIiIpVcKDVhY51zncMeSZRJTkgulz5hAC1bQu/e8Pbb5bI5ERERqQJCScL2ByY552Y756Y656Y556r0ZYug/PqEBQ0a5M+QnDWr3DYpIiIilVgoSdhRQBvgCOA44NjAbZVWns2RAP/5D8TEqDZMREREvFCSMCtmqtLKszkS/FmSBx8Mb70FVuXfPRERESlNKEnYZ8DowO23wFzgi3AGFQ2S45PLtTkSfJPknDkwcWK5blZEREQqoVKTMDPrbGZdArdtgH2AceEPLbJSElLKtTkSYOBASEjwtWEiIiKyewulJmw7ZjYZ6B2GWKJKOGrCatWCo4+Gd9+F3Nxy3bSIiIhUMqUmYc65awtM1zvn3gKWhLJx51z/wFmV/zjndhgv3jn3f4GzLac45352znXYidcQFskJyWzN3UpOXk65bvf002HpUvjhh3LdrIiIiFQyodSE1SgwVcP3DTu+tCc552KB4fizKzsApxeRZL0VaObsBjwAPBJ66OGVHJ8MUO61YcceCykpOktSRERkd1dsEuacGwxgZncC75vZnWZ2j5m9aWZbQtj2PsA/ZjbXzLKAdyiUvJnZhgIPk4misy5TElIAyr1fWPXqcMIJ8P77sHVruW5aREREKpGSLlt0PvBU4P7rQI8ybrsxsLDA40UU0ZfMOXc5cC2QABxS1IaccxcDFwM0aNCA9PT0MoZSNps2beLf5f5Cj9/99B1Nqjcp1+136lSHN97owkMPTWO//VaX67aruk2bNoW9/KXsVC7RS2UTnVQu0asiyyaUa0cCuHAFYGbDgeHOuUHAbcA5RawzAhgB0KtXL+vXr1+4wgEgPT2dXq16wZ/QsXtHujfsXq7b328/ePBBmDq1M7feWq6brvLS09MJd/lL2alcopfKJjqpXKJXRZZNSUlYLefcifgmy5rOuYEFF5rZh6VsezHQtMDjJoF5xXkHeKaUbVaY5IRAn7Bybo4EiI/3Y4Y9+ywsW+YHchUREZHdS0kd838ABuAvU/Qj/lJFwenYELb9G9DGOdfCOZcAnAaMKriCc65NgYfHAH+HHnp4besTVs4d84OuuAKys30iJiIiIrufYmvCzOy8XdmwmeUEOvd/BcQCL5nZDOfcMGCimY0CBjvnDgOygbUU0RQZKdvOjgxDTRhAmzZwzDHwzDNw881QrVpYdiMiIiJRKtQ+YTvFzD4HPi807/YC968K5/53RbA5sjyvH1nY1VfD4YfDO+/AOVGTfoqIiEhFKPOI+buLcI0TVtChh0LHjvDYY7qot4iIyO5GSVgxwjVOWEHOwVVXwZQp8OOPYduNiIiIRKFQLltU3Tk3xDn3fOBxG+dcKB3zK7Xq8dWB8NaEAZx5JtStC48/HtbdiIiISJQJpSbsZWAr0CfweDFwd9giihKxMbEkxiWGtU8YQFISXHwxfPwxzJsX1l2JiIhIFAklCWtlZg/gz2DEzDIJ4+Ct0SQ5PjmszZFBl10GsbHw1FOlrysiIiJVQyhJWJZzLonAdR2dc63wNWNVXkpCSoUkYU2awCmnwAsvwMaNYd+diIiIRIFQkrChwJdAU+fcm8C3wH/DGVS0SE5IDnufsKCrroING+CVVypkdyIiIhJhpSZhZvY1MBA4F3gb6GVm6eENKzokxyeHvU9YUO/esO++voN+Tk6F7FJEREQiKJSzIz8FjgDSzWy0ma0Kf1jRoaKaI4NuugnmzFFtmIiIyO4glObIh4ADgJnOufedcyc75xLDHFdUqMjmSIABA3xt2J13wpYtFbZbERERiYBQmiN/MLPLgJbAc8B/gBXhDiwaVNTZkUHOwb33wqJF8PTTFbZbERERiYCQRswPnB15EvB/wN7Aq+EMKlpUZJ+woIMPhiOO8MnYhg0VumsRERGpQKH0CRsJzAIOAZ7Cjxt2RbgDiwYpCSkV2hwZdO+9sHo1PPxwhe9aREREKkgoNWEv4hOv/zOz780sL9xBRYvkhIptjgzq2RNOPhkeeQRW7BYNvyIiIrufYpMw59whgbvJwPHOuYEFp4oJL7KS45PJycshKzerwvd9112QmelrxURERKTqKakm7KDA7XFFTFX+At7ga8KACu8XBrDXXnDeefDMM7BgQYXvXkRERMIsrrgFZnZH4O4wM9vu0tLOuRZhjSpKpCSkAJCRlUGdpDoVvv877oA33vBDVrz0UoXvXkRERMIolD5hHxQx7/3yDiQaJcf7mrBI9AsDaNoULr8cXn0VJk2KSAgiIiISJsXWhDnn9gI6AqmF+oDVBHabwVqBiJwhGTRkCLz1Flx8MUyYAHHFlpiIiIhUJiXVhLXD9/2qxfb9wXoAF4U9sigQrAmLRJ+woFq1/PUkJ0+Gp56KWBgiIiJSzkrqE/YJ8Ilzro+ZjavAmKLGtj5hEWqODDrlFN8kedttcNJJvplSREREKrdQ+oT9n3OuVvCBc662c2636CYeDc2R4C9nNHw4mMHgwf5WREREKrdQkrAuZrYu+MDM1gLdwxZRFIl0x/yCmjf3Z0mOGgUffRTpaERERGRXhZKExTjnagcfOOfqUEIzZlUSyXHCinL11dC1K1xxha4rKSIiUtmFkoQ9DIxzzt3lnLsLGAs8EN6wokPBccKiQVwcjBgBS5fCrbdGOhoRERHZFaUmYWb2GjAQWB6YBprZ6+EOLBokxSXhcFHRHBm0zz6+X9jw4fDjj5GORkRERHZWKDVhAHWADDN7Cli5u4yY75yjenz1qKkJC7r3XmjdGgYNgtWrIx2NiIiI7IxSkzDn3B3AjcDNgVnxwBvhDCqaJCckR02fsKCUFHjnHVixAs4/X2dLioiIVEah1ISdCAwAMgDMbAlQI5xBRZOUhJSoao4M6tEDHnzQny2pQVxFREQqn1CSsCwzM8AAnHPJ4Q0puiTHJ0dlEgZw5ZVw7LFw/fXw+++RjkZERETKIpQkbKRz7jmglnPuIuAb4PnwhhU9khOSo65PWJBz8PLLUK8enHYabIquVlMREREpQShnRz4EvA98gL+e5O1m9mS4A4sWyfHR1yesoHr14M034e+//VmTIiIiUjmENOiqmY0BxoQ5lqiUkpDCysyVkQ6jRP36wZAhMGwY7LcfXLRbXF5dRESkciu2Jsw593PgdqNzbkMR0zzn3GUVF2pkRHNzZEFDhsCRR8Jll8H330c6GhERESlNsUmYme0fuK1hZjULT0Av4KqKCjRSor05MiguDt59F9q2hZNOgr/+inREIiIiUpKQBmt1zvVwzl3pnLvCOdcdwMxWA/3CGVw0iOazIwtLTYXRoyE21p81uWZNpCMSERGR4oQyWOvtwKtAXaAe8Ipz7jYAM1sa3vAiLyUhhYysDKySjIjaogV8/DEsWAAnnwxZWZGOSERERIoSSk3YGcDeZnaHmd0B7AucFd6wokdyQjKGsSVnS6RDCdl++8GLL/q+YZdfrhH1RUREolEoZ0cuARKBYBZSDVgctoiiTHK8H5t2U9YmkuKTIhxN6M48E2bPhrvvhmbN4LbbIh2RiIiIFFRsEuacexI/Sv56YIZzbkzg8eHArxUTXuSlJKQAkJGdQRppEY6mbO680zdLDhnirzd59dWRjkhERESCSqoJmxi4nQR8VGB+etiiiULJCb4mrDIMU1FYTAy89BJs3gzXXANJSXDJJZGOSkRERKCEJMzMXgVwziUCrQOz/zGzytM5qhwEmyMryxmShcXF+RH1N2+GSy/1idjZZ0c6KhERESlpsNY459wDwCL82ZGvAQudcw845+IrKsBIC9aEVYaxwoqTkADvvw+HHALnnQfvvRfpiERERKSksyMfBOoALcysp5n1AFoBtYCHKiC2qLCtT1glbI4sKDERPvkE+vaFQYP8MBYiIiISOSUlYccCF5nZxuAMM9sAXAocHe7AokVlb44sKDkZPvsMevb0Y4i99lqkIxIREdl9lZSEmRUxQqmZ5eLPktwtVOaO+UWpWRPGjPEX/T7nHHjssUhHJCIisnsqKQmb6ZzboQu3c+5M4M/whRRdCo4TVlXUqOFrxAYO9GdNDhmiAV1FREQqWklDVFwOfOicOx8/TAX4i3YnASeGO7BoUXCcsKqkWjUYOdIPWXH33f46k08+6Ye1EBERkfAraYiKxUBv59whQMfA7M/N7NsKiSxKJMQmEOtiq0xzZEGxsfD881C3LjzwAKxYAa++CtWrRzoyERGRqq/UyxaZ2XfAdxUQS1RyzpGckFzlasKCnIP774cGDeD662HuXH8WZZMmkY5MRESkalPjUwiS45OrVJ+wolx7LYwaBX//DXvvDRMmRDoiERGRqk1JWAhSElKqbE1YQcceC+PG+VH1DzrIj7QvIiIi4aEkLATJCclVsk9YUTp2hF9/hd694cwz4aabICcn0lGJiIhUPUrCQpAcX3X7hBWlXj0/ltgll/j+YoccAosWRToqERGRqkVJWAiSE6p+n7DCEhLg2Wfh9ddh8mTo1g2++CLSUYmIiFQdSsJCkJKQsts0RxZ25pkwcSI0agRHH+2bJ7OzIx2ViIhI5ackLAS7W3NkYXvt5c+WvPhi3zx50EHwzz+RjkpERKRyUxIWguT43adjfnGSkuC55+Ctt2DmTOjaFYYPh7y8SEcmIiJSOSkJC8Hu2CesOKefDtOnwwEHwODBcMQR8O+/kY5KRESk8lESFoKUhBQ252wmz1TtA340/S++gBEjfDNl587w0ku6CLiIiEhZKAkLQXJ8MgCZ2ZkRjiR6OAcXXQRTp0L37nDBBX4oiz//jHRkIiIilUNYkzDnXH/n3Gzn3D/OuZuKWH6tc26mc26qc+5b51yzcMazs5ITfBK2u/cLK0qLFvDdd76/2JQp0KULDBkCmzdHOjIREZHoFrYkzDkXCwwHjgI6AKc75zoUWu13oJeZdQHeBx4IVzy7IlgTpn5hRYuJ8WdOzp4Np54Kd9/tmyi//jrSkYmIiESvcNaE7QP8Y2ZzzSwLeAc4vuAKZva9mQXb+MYDTcIYz05LSUgB2K2HqQhF/fp+cNdvv4XYWDjySDjhBH9RcBEREdleOJOwxsDCAo8XBeYV5wIgKsdkV3Nk2RxyiO8r9r//+YSsY0e49lpYuzbSkYmIiESPuEgHAOCcOxPoBRxUzPKLgYsBGjRoQHp6eljj2bRp03b7+GvdXwCMnTiWrXO2hnXfVcm++8Krrybw0kvNeeyxhrz4Yg7nnjuP445bSlzczp1KWbhsJDqoXKKXyiY6qVyiV0WWTTiTsMVA0wKPmwTmbcc5dxhwK3CQmRWZ4ZjZCGAEQK9evaxfv37lHmxB6enpFNxHneV14A9o3KYx/TqFd99V0cCBvtP+tdfG88QTbfnss7YMGwanneb7k5VF4bKR6KByiV4qm+ikcoleFVk24WyO/A1o45xr4ZxLAE4DRhVcwTnXHXgOGGBmK8IYyy7Zq95eVIutxsQlEyMdSqXVrZtvmvz0U0hOhjPO8PM+/VTji4mIyO4pbEmYmeUAg4GvgFnASDOb4Zwb5pwbEFjtQSAFeM85N8U5N6qYzUVUQmwCPRv1ZNyicZEOpVJzDo49Fn7/Hd5+2w9jMWAA9Onjz6RUMiYiIruTsI4TZmafm1lbM2tlZvcE5t1uZqMC9w8zswZm1i0wDSh5i5HTp0kfJi2ZRFZuVqRDqfRiYnxT5MyZ8PzzsHixP5Oyd28YNUrXoxQRkd2DRswPUZ8mfdiau5Xfl/4e6VCqjPh4uPBC+Ocffwmk1avh+ON9M+W770JubqQjFBERCR8lYSHq07QPgJokw6BaNX8JpNmz/Thj2dm+pqxdO3jqKcjQyCAiIlIFKQkLUaMajdgzdU8lYWEUFwdnngkzZsD770NaGlxxBTRtCrfcAkuXRjpCERGR8qMkrAz6NOnDuIVKwsItJgZOOgnGjYNffoGDD4b77oNmzeDee/diwgR14hcRkcpPSVgZ9GnSh4UbFrJow6JIh7Lb6NsXPvjAX/rokkvg55/rse++sPfe8MorulC4iIhUXkrCymBbvzDVhlW4Vq3gySfhvffG8dRTkJkJ550HTZrA9dfDn39GOkIREZGyURJWBt326EZiXKL6hUVQcnIul1/u+419951vqnz8cWjfHg44AF591SdoIiIi0U5JWBkkxCbQs6EGbY0GzvkE7P33YeFCuP9+WL4czj0XGjaE//s/36dMfcdERCRaKQkroz5N+jB56WS25uhC3tFijz3gv//1Q1z88IMfa+y113x/srZtYdgwmDs30lGKiIhsT0lYGfVp2oes3CwmL50c6VCkEOfgwAN9ArZ8Obz8Muy5Jwwd6vuU7b8/DB/ul4mIiESakrAy6tNEg7ZWBjVq+KbJb7+FBQvgf/+Ddetg8GBo1AgOPxxefBHWro10pCIisrtSElZGDWs0pFlqMyVhlUjTpnDTTTB9Okyb5gd+nT/fXzKpfn3o399fw3LlykhHKiIiuxMlYTuhT1MN2lpZdeoEd90Ff/0FEyfCNdf4Mcguvtj3LTv4YD8Uxr//RjpSERGp6pSE7YQ+TfqweONiFq5fGOlQZCc5Bz17wgMP+AuIT5kCt94KK1bAlVf60fm7d4c77oBJk3SWpYiIlD8lYTtB/cKqFuega1d/FuWMGX7g1wcfhJQUuPtu6NXLN2lecgl8/DFs3BjpiEVEpCpQErYTuu7R1Q/aqibJKqldOz8K/08/wbJl/vJI++4Lb78NJ54IdevCYYfBww/7pE21ZCIisjOUhO2EhNgEejXqpZqw3UBaGpxzjh8UdtUq+P57uPpqn5xdf73vY9akib+E0ttvq3O/iIiETknYTgoO2rolZ0ukQ5EKkpAA/fr5fmTTp/uhL55/3o8/NmoUDBrkz7bs3h2uuw4++ww2bIh01CIiEq2UhO2kPk36kJ2XzaQlkyIdikTInnv6YS7efdd36P/1V9+HrE4dPyjsscf6+336+GExvvpK/clERCSfkrCd1LdpXxyOZyY+g6lT0G4vNhb23tufYfntt35g2O++g5tv9h3/H3zQj0dWuzbssw/ccAN8+imsXh3pyEVEJFKUhO2kBikNuOOgO3hz2psM/214pMORKJOY6Mccu+suGDvWJ2Vff+0Hja1WDZ54AgYMgHr1oGNHf+bl66/7a1wqpxcR2T3ERTqAymzIQUOYtHQS13x1DV0bdOWAZgdEOiSJUsnJ/lJJhx/uH2/eDL/9Bj//7Kd334URI/yy+vV9E2afPv6szL33hurVIxe7iIiEh5KwXRDjYnj9xNfZ54V9OPm9k5l08SSa1GwS6bCkEkhK8hcbP/BA/zgvzw938csvMG6cnz75xC+LjYXOnX0z5j77QO/e0L69ny8iIpWXkrBdlJqYykenfkTvF3pz0siT+PHcH6kWVy3SYUklExPjE63OneH//s/PW7UKxo/306+/wsiR+bVlycnQo4cfSLZnT3/bpo3fjoiIVA5KwspBh7QOvHrCq5w08iQGfz6Y5wc8H+mQpAqoV8+fYXnssf5xXp6/xNKvv/pp4kR45hnYEhglpUYNPzxG9+4+Qeve3deYxelbLiISlfTzXE4Gth/ILfvfwr0/38uBzQ7krK5nRTokqWJiYqBtWz+deaafl5MDM2f661v+9hv8/ruvLdu82S9PTPQDynbt6qdu3aBLF0hNjdjLEBGRACVh5WjYwcP47O/PeHzC40rCpELExfmkqksXP2o/QG4u/PUXTJ7sk7IpU3z/shdfzH9es2b+OcEm0C5dfHNmfHxEXoaIyG5JSVg5io2J5eyuZ3Pd19cxe9Vs2tVrF+mQZDcUG+ubIdu3hzPO8PPMYOlSn5D98QdMnQrTpsHnn/ukDXwCttdevuasY8f82xYtdBKAiEg4KAkrZ6d1Oo3rv76eN6e9ybCDh0U6HBHADxjbqJGfjj46f/7WrfDnnz4pmz7dn6E5dqy/DmZQtWr+ouYdOuQnd+3bQ+vWvrlTRER2jpKwctaoRiMOaXEIb057kzv73YlzLtIhiRSrWrX8/mIFbdzo+5rNmAGzZvlpwgQ/nllwMNmYGF9LttdekJzcir/+8sla27awxx4+8RMRkeIpCQuDMzqfwfmjzmfC4gns22TfSIcjUmY1avjxyHr33n5+ZibMnu1rzwpOs2Y1ZuTI7Z/ftq3vZ9amzfb369Sp2NciIhKtlISFwcD2A7n0s0t5c+qbSsKkSqlePX8YjIK+++5HWrbsx19/+ZMCZs/2txMm+PHN8vLy161d2zdltm4NrVrl37Zs6WvQNNaZiOwulISFQWpiKse1O453Z7zLI0c+QnysTjmTqi0mBpo399MRR2y/bOtWmDfPJ2V//w1z5vjxzoLNmwUTtMREn4y1bOmbOoNT8HGNGhX5qkREwktJWGFmJC1c6E8Z24VTws7ofAbvz3yfb+Z+w1FtjirHAEUql2rVfL+xvfbacVlWFixY4C9cPmeOvw3e/+EH3zetoLp185O94NSsWf6k8c9EpDJRElbY22/T++yzfU/lLl12ejNHtT6KWom1eHPam0rCRIqRkJDfV6wwM1izxidl8+b5af58P82YAZ99ln+1gKDU1PyEbM8986emTf1tw4a6goCIRA/9HBUW7Ik8duwuJWHV4qrxnw7/4c1pb5KRlUFyQnI5BSiye3DO13zVrQt7773jcjNYscInZQsWwL//+tvg9PPPsHbt9s+JifGJWNOmfmrSJP+2SRNo3Ngv16C1IlIRlIQV1rIlWbVrk/DLL/lXUt5JZ3Q5gxGTR/DJ7E8Y1HlQOQUoIuCTtAYN/FT4LM6gjRth4UKfoP37Lyxa5B8vXOgHrR09Ov8ST4W327jxjlNwrLVGjfwJBhqGQ0R2hZKwwpxjfadOpP3yyy5vav8996dpzaa8Oe1NJWEiEVCjhh9ktkOHopebwbp1PjkrOC1e7Kd583yN2po1Oz43MdHXmjVs6JOy4P3gtMce/rZePZ3xKSJFUxJWhPWdOpH200/+Oi8NG+70dmJcDIM6D+KhsQ+xMmMlaclp5RiliOwq53yNVu3a/hqaxdm82f8cLFmSPy1enD9v+nQYMwbWr9/xubGxUL++T8oKTsFavIJTnTqqXRPZnSgJK8KGTp38nbFj4aSTdmlbZ3Q+g/t/uZ+3p7/Nlb2vLIfoRKSiJSXlD51RkowMWL7cJ2fLlm1/u3y5vz9tmr/Nydnx+XFxPmGrX98nZcHbtLT8+fXr+8dpaX7cNhGpvJSEFWFj69b+vPpfftnlJKxzg87s3Whvrv3qWuatncedB99JzWo1yylSEYkmycmhJWt5ef6kgWBitnx5/rRiRf7trFn+/tatxe8vmJAVnOrVy79duLAmjRv7Exxq1VLTqEg0URJWBEtI8KdjjR1bLtv78swvufXbW3l8wuO8M+MdHjr8IQZ1HqTrSorspmJi8s/8LK6/WpAZbNoEK1f6xCw4rVy5/RSsZVu5svDQHT122G+9ekXfBqc6dba/r7NFRcJDSVhx9tsPHnnEdwZJStqlTdVJqsMzxz7DBT0u4LLPLuPMj85kxOQRPHT4Q/Rq1EvJmIgUyzl/gkGNGqXXsAVlZOQnZ99/P5VGjbqwahWsWuXnrV7tp+CVC1atguzs4rdXo8b2yVnt2v5+cCr4OHi/dm3/06mfN5HiKQkrzn77wf33w2+/wYEHlssmezXqxfgLx/Pi5Be56dub2OeFfWhRqwUD2w9kYPuB7NtkX2Kc2gpEZNckJ/upeXPIyFhDv34lrx+sbVu92p8JGkzSgo8Lz1uwwDenrlnjLy5SnISE/BMfCk+1au14W3CqWXOXLloiUikoCStOnz7+duzYckvCwJ8xeVHPizi5w8l8MOsDPvrzI56Y8AQPj3uYPVL24OT2J3NRz4vo0mDnB4oVESmLgrVtzZuH/jwzPxbbmjX5SVnwNni/4LRsGfz5p7+/bp1/fklq1vRXQQgmZsH7BW9LmpKTVRMn0U1JWHHq1YN27Xzn/DConVSbC3tcyIU9LmT9lvV8/vfnfDDrA56f/DxP/fYUfZr04ZKel/Cfjv8hKX7XmkNFRMLBOZ8o1axZtuQN/MkJGzfmJ2QFp7Vr/XAfwcfB+4sW+UtWBR8XvPh7UWJi8uNLTS36tqipRo3t76ek6IQGCQ8lYSXZbz/4+GP/TQ/jNzA1MZXTO5/O6Z1PZ3Xmal774zWem/Qc535yLtd8dQ0X9biIuw+5m/hY9Y4VkaohJia/xmpnmPm+b8EkrfC0YcP2t8H7y5fD33/nzy98/dHipKTkJ2fBWsOCU8H5wXWLu7+L3YylClESVpK+feGll+Cvv2CvvSpkl3Wr1+WaPtdw9b5X88OCH3h24rM8MPYB/t3wL2+c+AaxMeokISLinE9qUlL8dT93VlaWr5HbsCE/Mdu4MX9ewdvC8+bPz5+/cWPxQ4kUFhMDSUn7k5qa/xpKmpKTi34c7PsXfJyQsPPvg0SGkrCS7Lefv/3llwpLwoKcc/Rr3o9+zfvRq1EvbhhzA9Viq/HS8S+p876ISDlJSMgfjmNXZWf7ZGzTpu2Ts+C84LRxI8yatYzatZtse7xpk+8zl5Gx/bolnfhQWFzc9olZKFP16jveLzwvOMX9OR2uuQZGjIAWLXb9DRMlYSVq185/M3/5BS64IGJhXN/3ejKzM7kj/Q6S4pJ4+pinNayFiEiUiY/PH6qjNOnp/9CvX8lVeGa+di2YmBVM0ArPy8jY8X5wWrfOX2ar4LzMzLK+OiPdDeYg+4EPO97GjY3f3C5BC05JSaU/Ds4reFvwfkLC7nNChZKwkjjnmyTLadDWXTHkwCFszt7Mfb/cR2JcIo8c+YgSMRGRKsw5SJz0C4k9elC3bvl2JDPzw2AGE7LCt4XvN/ntY/b+4Ace3LchV/7yFr+2uZHZ1bpsW2/duvx1g9PmzaWfAVvc6w4mZmWdEhNLvp+YuP39WrUie/kvJWGl6dsXPv3Uj2ZYr17EwnDOce+h95KZncljEx7DOccxbY7ZbrnDUS2uGklxSSTFJ5EUl0RyQjL1qkcu7sLMTMmjlGzdOrjxRt/R56abdvvh2ldlroqq77BUoK++gv794YgjYNQofzm9cuJcfu1UqbZuhY43cOvAetzbfil1c5O4L34IfPRJiU8L1uQFE7KiboP3g1Phx0XNX7Fi+2VbtvjbUPvkFXTfff7nJlKUhJUm2C9s7FgYMGD7ZTk5vhG+gjjneKz/Y2zJ2cKj4x/l0fGPhvS8I1sdyaNHPkr7tPZFLv/qn6+47fvbyMzO5PCWh3N4y8M5qPlBpCSk7Lhy8BtVu3aZYs/Ny+XBsQ9y70/38tqJr3HCXieU6flhsWgRPP00XHdd+XQIkV33889wxhm+bPLyYPRoeOMNaNMm0pFFxCd/fsIJ757ALfvfwt2H3F0+f2BefNEfsS6/fPdp8wmH55/3v//nnRe+fTzwgD+d8uuv4cwz4Z13Qh/BdvlyuPZaaNYM7r131+J46ikWr5jDox2rQR48c0Qdzh86CsaPh333LfZpzuXXPFWEvLz8hKzwbTBJK7xsn30qJrZimVmlmnr27Gnh9v333+c/yMw0i483u/HG/HnZ2WYPPmhWvbrZ//2fWU5O2GMqKC8vzyYunmg/zv/Rfpj/w7bp+3nf2xd/f2EfzfrI3pr6lr04+UUb+v1QS/1fqsUNi7Orv7ja1m5eu207U5dNtSNfP9IYirV8vKUd8foRlnh3ojEUix8Wbwe9fJDd//P9NmfNHP8+PPGEWaNGZqmpZuPHhxzvwvUL7eBXDjaGYjX/V9Pq3l/XlmxYslOvfbuy2RV5eWaHHWYGZh07mi1eXD7b3U3tcrlkZ5vdcYdZTIxZq1ZmEyaYjRxpVru2/56NGOHLrCr76Sezs882W7nSzMyycrKs7ZNtt30nr/vqOsvbifdgu7L59Vf/HoPZf/9b+d/TjAyzWbPMcnOLX+fvv81uuMGsRw+zn38un/3ee69/D8Hs1lt36n0s9TszcaLf/gMPmD3yiL9/wQWh7evDD83S0vJj/OijMse3zYoVZqmpduFlTSx+WLxd++W1xlDs1461zA49dOe3W5RNm3wZPfaY2XPP+ccRUG7HmQBgohWT00Q8qSrrVOFJmJlZ795m++/v70+ZYtazp3/runb1tyefbLZlS9jj2lkrNq2wi0ddbG6os3oP1LMnJzxpF35yocXcGWO176ttj4x9xLZk+/g3Z2+2b+Z8Y//9+r/W7dluxlCModjel8Xbg32x+YfvbXktW9jcJin23kf32M3f3GxHvH6E9Xiuh1331XX23dzvLCsna9u+P5j5gdW+r7Yl35NsL01+yWatnGVJdydZ/zf67/oBZVe8/LIvu4suMktJMWvRwuyff0J/fk6OTxRWrCifeMzMvv/e7Jprth2Eo9J335ldcYU/KHzxhdmCBWZ5ebtWLvPmmfXt68vj7LPNNmzIX7Zwof+hB7Pjjy/f9zuajB5tlpjoX+cBB5ht2WLP/PaMMRT75M9PbPBng42h2BWfX1Hm7822stmyxf/haNzY7MIL/b6uuqpyJmKbN5s9+qhZ/fr+ddSvb3bmmWavv262fLlZVpbZe+/l/9GKjTWrV88sOdnsxx93bd/33++3OWiQ//0As0svLTkRLEKp35lTTzWrWdNs3Tr/eMgQv69rry2+zNatMzvnHL9ejx7+eNWjh3/tS5cWv68NG/y6Rbn0UpvRIMZi7oyxKz+/0tZvWW/J9yTbeffu4/fzzTelvdTi5eWZvf++2XnnmXXqlP8HITjVrWs2dKjZqlU7v4+doCQs2pKwa681q1bN7OabzeLi/Bd+5Ej/AXr4Yf82Hnro9gePoB9+MDvuOLP+/c2eesoftHbV1q1mY8eW+Us/ed5YO+D+vXxN1+3Orr6xm61++C7/r+n33/30+edmL75odvfdZpddZvNa17MH+2J7X1tjW0JW8578+3F3xlq3Z7tZv1f6Wfyw+G21XSff18POvHQPYyjWa1gT++v958zmzjXLzbXhvw43hmJPTXiq6EDnzDE78ECzzp39e3fllf6g/+GH9vOu/KMLWrbM17Dsv79/D3/91X/Z99jDbOrU4p+3ZYt/fy68MP9fZmKi2SWXmM2evWsxvfii/2yBWcOGZl9/vWvbK2+Zmf6ADf67UPCHMjnZNrZoYda+vVnLlr62tE4dswYNzN59t+Ttjhvny6JmTbM33yx6ndxcX/4JCX4f5fXvODvbJ5Vz5kQ2EXnzTV/2PXr43wiwjecOsgYPNrADXjrA8vLyLC8vb1sNxMWjLrbcvNC/+9t+z26/3ZfX6NH+9V59dckJxOzZ/nempPcmL8/shRfM9tqr+PIrT1u2mA0f7j9jYHbIIWZPP212+un+Oxz8TKam+tumTc2GDfM13UuW+DiTk83S03du/w895Ld72mn+85OX52sUwceQlVX6NgJKPNDPnesTkhtuyJ+Xl+f/AIHZXXf5xxs3+nXHj/fv/557+oRzyBB/nDAzmznT/04dfXTRZblkiU+AwP8mfv55/nrTp5vFxNiAm5tbjXtr2IpN/k/QxaMutqS7k2xNq8a+kmJnvj8TJuT/+UpL8/HdfrvZqFG+vH7+2f/+g68Nv/JKP++DD/xx94orzAYM8In2Aw/4Gs9yoiQs2pKwDz7I/3Kfe67Z6tXbL3/1Vf/B79XL/1PPyzP76iv/jzb4L61Nm/xtdOniq7BHj/b/yqZM8V+klSv9F7skP/3kD0Rg1q9f6bU3eXn+g3vRRWY1a1oe2A97p9mcfdv5GqCCB9PCU61aZkcc4RNJM5uzZo7d99N9dunoS+25b+6z33o3tS2pyduq+DdkrrOPRlxrF55R0xpdi7k7sBuPT7atsQW2mZJieXfdZUe9dqQl3p1oM1fM3D7e777zB/Datc2OPdYnYgXizK5e3X/htmyxzKxM++LvL+yDmR/YovWLSinVAk45xR/QZ83Knzdjhq8hqF3bJwaZmWZ//OGT7WHD/HNq1vRx1Kjhf3Bff93s4ot9UuKc2Qkn+PeiLD9Iubk+uYf89zpYvtdc4//xFyU7u8xJ+DYbNvgf3vXrQ2tKnzgxP6Yrr/TvzYoVPtZnnzW78kpbud9+/j0680yfpA4ebLb33v59efrporc7Zow/ILZqFVot5JgxfnvnnVe211uUdevMjjoq/3PZsKGP//HHzSZPrrikbPhw/5oOOsiXh5nZHXfY0IP8n5xxC8dtWzUvL89u/uZmYyh27sfnWk5ugbKbN8/srbfM5s/fYRfff/+9/42JizM766z8BXl5vptFsJkrJ8e/9iFDfI1Z8L058kizv/7aMfbVq81OOsm21ViA755R3Gd2V8ye7b/3e+7p97Pffv63oqDcXLPffrOPhp1u3W+uY7+PfGLHz/fSpWYdOviDeuHnl+bRR/2+Tzllx9/p++7zy44+2jeRhqDEA/3gwb4bzKJCv2u5ub4MwSwpacff7DZtiu4q8uSTfvkzz2w/f84c/8cpJcXsllvMmjTx63Xv7msSjzzSfmqfbAzF7v7h7m1P+33p78ZQ7NFHT/Xrf/JJSK/ZzMz+/dfsjDP88xo08H9AS/odmj7d1+4F/6QWOJZYp07+GBGc16mT2W23+fdgwQL/GS1DYhykJCzakrCNG/2P1FdfFf+kUaP8v422bf3BB/wH+skn/UHLzOzPP31fsgMP9ElbUYlPaqr/Ifv11+0PBGvW5DchNGvm/zHUrOm/iA8/vOOHeMEC/2+pdWsL1lbY2Webfftt/sE7L89X806c6L9w779v9ssvPiEMxlySRYv8601J8Qevbt38vjp3trxRo2zD5vX5sf/yi9nzz/tEBWxJr3ZW955U6/FcD9uaE/jHNny4f1/at9/+oJyX579MY8fa+EO72eO9sf4XVbfEOxO21cgxFGv6SFM7ZeQp9vDYh+3H+T/a6szVO8b88ceW67CZd11pL//+st39w922bOMyv2zePP9+xcX5A2OBctnQsonlnn+eT5wLH2SWLfMHrjp18pOpIg6GO8jM9D/o4GvTgj8WGRlml1+en7CPH+/L7cEHffLXrl1+fHFx/oBSq5avyTv0UJ/gf/ppftPd5s3++bfcYrbPPjtW+Scm+oNohw7+oHrbbbbl9Vfs/dEP2jt3/sfvo3HjEmvnivzRyszM/yc7dOj2n+ePP/aJcKdOJTeTFJCVk2VbbwskrG+8UfyKr7/u91ug2emvVX/ZkO+G2LrN6/yBp0MH/7oeftgniYMG5R/ggzXbRSUepcnLCy05zsvz30/w/+YLfN+WbVhqybfH2Un/wf8B3O5peTb0+6HGUGyf+1vblMtP9klswQPTiy9u916njxnjD6oNGuz4BzIvL7+GrHZtfxsT4//gPf64f39q1PBldeut+QnGd9/5z0R8vE+Otm7NrxHq0cP/huyKnBz/h/OGG/znPfj6+vQx+/LLYpPk2atmW417fU19rftq2fiFRSQky5f7z11Skm9K27zZ/94+/bT/ne/Vy/+G9+/vk4UrrvB/tsB/P4o7qD/3nC2ugf16WHvLmz691JdY7IF+1Sr/nT733KKXZ2eb3XOP2XXX+abRl182++wz/xqK6xaTm+t/l6pXz6+1nzrV//moU8fXSpn5cnzxxW2VBnlg+w5rZg0famibtm5fA93nhT7W9om2ltemtX8/S/vc5+b6uJOS/B/XW24puvWoOAsX+mRv8mT/OS74GZg3zyfJBx204+9bsPY+Lc1/fi64wNesf/ml32YRnyUlYdGWhIXqp5/8wbBlS59wBKuDi7J6tT+4jhnjf2hfftl3fD/zzPx/OB07+urvl1/2tWmxsWbXX5/fHLNwoa8tAl8l/Ntv/gB06KH5B+l+/cxeecUnkuGweHH+j2TLlv7gWNqX8ZNPzBo3tg/b++TpltHX+sQTzI45Jr9GwMxy83Jt/MLxduOYG63tk223JVxtr02wq/pjX/6np41/6S57/KFT7LShnaz5kJrbJWYNHmxgB79ysF3+2eV26+fX25EXVLNat8Rut06t+2rZM78945t4li3zNVBDh5q9/bYtHPulXfjBuRZ7Z6zVvq+2nfDOCfbYuMdsytIpOzYJbdrkfwhSUvz09NPFvxfz55vtu68vp4ceKvqgMnp0fp+X4LTnnr5v1K23+hhvvtnsuuts/lXn2GNX7mOzDuywfYLfvHl+X6PYWF/9P2SI/0f80ENmd97pD3SXXWZ5A46zcfs2sf87Fqt9Y/77c//lXX0iXYx5a+fZRa9e5BOcwrKz/cEEzC67zB9cX3vNx7LPPjsmBcWYuWKmtXmijbV6vJX9dXhP//4WTpLy8vyPPPjkAMyOPdb++Ol9q/9gfWMo1v2hNrasaaCm9dtvd9zRggWW9+ij/g9OtWq+Wb6I73FGVoZd/cXVdsd3t9va337yCcuJJ/oDWmysP7h16+YP5Oec4w/iJ57om3vatbMVjWvZpIZY3lln7lCrctnoyyz2zlibfXAX/1swcaKPIT3dH7h69bK3OmNpN2Cxt2M3XNrKNj32gE86+/Xzr/u44/xn2czmBP+8FUrotvPUU75v64sv7tjvbskS/7sU/AN40UX+c9u2rY+toE8+8b+BtWqFVDuSm5drExdPtOWblvsZK1f6Tu9Nm+b/yTjsMP9ntpQ/NhlZGdb56c5W9/669tOCn6zl4y0t5d4U+2H+DzuuvGKF/4MTH79dDcs/LWpZz+tqWN1b463OLbFW6+YYq3kzVvMm7JRrm9qSNcV3J3njjzesxrAkYyjW5grs/qGH29LVRayfk2P21Vf29+WX5/f3KmjYMB9PCIlcmSxe7D+fe+/tj1W1a/tm3RkzbNH6RfbhzA9t6cal+TG+8459eOMAYyg2YuKIHTb32pTXjKHYNy/cattqQYs75m3Z4ptwg4nsvHnl+9oKWrHCVyi88IL/PR42zP9BuOgin6QVPGEB/PJCKjIJc3555dGrVy+bOHFiWPeRnp5Ov379du7Jmzb583F3ZeiK9eth5Eh/3crx4/28vff2l4ro1m37dc3g7bfhyith9Wo/r0ULOPdcOPtsaN585+MI1cqV8OOPcNxxoV+8bMMGuOUWLlg0nFe6wol/Qv023Ug75FjSUhpQO7E24xeN56M/P2LxxsXExcRxcPODaR/TnquOvoqWKU1h+HAYOtS/X0GpqSxrWpvf3XJmpmxmZptUZrauxcy4NWzauonOy4199xnIvt2PY98m+2JmXP755Xw//3v2abwPzx7zLN0bdmd15mr+9/P/eOrXpzCMC7pfQFZuFunz05mzdg4AtRNrc+JeJ3Lp3pfSq1Gv/BgWLICLL/anlR90ELzwArRu7ed/8AG8/z6MG+dHC3zzTTjxxOLfp+XL/UXkW7XyZV9grDoz45eFv/DY+Mf46M+PyLM8EuMSuWf/O7iK3sRO+A0mToTGjeHQQ+HAA/1VhgvIys1i/KLxfD3na0bOGMnfa/4mKS6JgY0O4SzXjVcyfuGdVek8csQjXNPnmh3C+2nBTwwcOZBVmatoU6cNH576IZ3qd9p+JTM/3tcDD0Dv3jBhAhxyiH9dNWqU+lEZ/ddoBn0wiKT4JHLzcnFmjH41h95Jrf3QMdWq+Wu7XHmlH3Jk0CD/2Xj2WSa8cjf9T8ggOb46t9U4huvWv0ejrfF8PegLWvQ4dLv9ZOdm8+SvT/K/n//Hyc2O4r4PN5L67sfQvr3/7nXqBLNmMWfaDwxc9AjTYlZiDmpvhht+gStWNCdlv4OhYUNfbsuX++vQLF/uz4WvX5/pzZN4rPVq3qj1L1tdLr0b9+aOg+6gf+v+OOf4e/XfdHi6Axf1uIine97u36/16/1wOBkZfniCPn3g8MNZ0683N619j+envEiz1GY8fczTHN2qPzzxhH+/a9SAW28l77//JeaEE/xvSojyLI+P//yYMXPGEBcTR0JsAvGLl5Lw1TfUWriSo7qeRPuHXvXXtSls3jw4+WSYPNm/d3365E/t22POMW3FNN6a9hZvT3+bf9f/S2pcCo8v68bZz/+K25rlPx8XXghHHx3SFb7NjPM+OY/X/niNL874giNbH8niDYs57PXDWLBuAR+f9jFHtDpi+yetWuV/P2rWhF69mNmqJod9czZZuVmc2vFUYlzMtikzK4PXpr1OYlwiDx/xMOd1O2/bcCEbt27k8s8v5/Wpr7P/nvtzdosTeP2L+/mp+kpi8+C4PQ7kksNv5kha4159FV55xQ/BAv67+dxzcExgzMfNm/2QEvvs44dnKW/vvw+nnOLvt24NY8bwde5fnP7B6azZvAaADmkdOLTFoRza4lBu/OZGnHNMu3QacTHbH9O25GyhySNN6NfsIN6f2BIeesgPV/Hee9tfzHPtWv8b98MPflCu//438kOjrFwJM2fCjBn+O9az53aLdykHKIJzbpKZ9SpyYXHZWbROUV0TFg4zZ/oakdL67ixf7purfvhh5/sKRcCGH762kwfXt73uaWR1769rbqjbVgOTeHeinfDOCfbalNdsTaavidmhbNau9X23lizZ/l/Yxo1mL73kq5/B8uLjbEssvgq/kLy8PHvjjzes/oP1LebOGDt55MlW8381LebOGDv343Nt/trt/4H/u+5fe/2P1+3sj8626vdU9ycfjOhlL05+0TKyMoIb9bUKqam+NqNHj/x/Xt27+xqbnWyyWbd5nb3y+yvW47kexlCs9n217cYxN9rExRNtwNv+n+sBLx1g/6zesZ9Vbl6uTVs+zR4b95gd8+YxlnyP7+8Rc2eM9Xuln700+SVbvyW/JjI7N9tOHnmyMRR7YvwT223rpckvWfyweGv7ZFu75Z1bbI+H9rDq91S3N/4opqnwwQfza2lC6DeUl5dn9/x4j7mhzno818P+XfevzV4121o+3tKS7kywT9rh+6ht3mw2cKDf9vXXb/v8fz/ve0u5J9la3p5q8+onmIGNPaGX1f5fLWv4UEObuiz/JIwf5v9gnZ7uZAzFej7X02LujLFGDzeyj1+/zdf+BMpudBus1o2+pvDzw5vb5IsH2LEP+LOI0x5Is0fGPmJLNy61VRmrbHXmalu7ea2t27zOPv/rczv8tcONoVjS3Ul2yaeX2BPjn7BmjzbzTYvP72OjZ4+2k0eebMn3JOc3kU+d6mvPLrvMN+EWqCUO+mnBT9ZheAdjKHbgywfaW1Pfsi1Tf9/2mcuqWXNbrVhpcnJz7O1pb1vH4R2NoVjq/1Ktzv11LOXeFKt2V7XtapA7DO9gt393u01dNnXHMzY3b/Y1rcccs62ZfnZd7O7Dq1nHq/wJPLG3Y0dfWN1eOLyuHXCe3+ZxNzWzpRPTQ4q1oBcmvWAMxW7/7vbt5i/ftNy6PtPVEu5KsFF/jir2+RMXT7S699e1hg81tOnLi66Bmr1qth348oHGUOzQVw+1OWvm2K+LfrVWj7eymDtjbOj3Qy07N79Wc9Z7T9sNA1Ms7Qb/2vqej/3Q3Pna0ZEjbfITT+T3vTvrLN8M+cwz/nHgxIHpy6fbVV9cZW9Pe3u7s85DkZWTZT8v+Nn+WPbH9gsGDzbr29dylyy2u3+429xQZ52e7mRf/v2l3f/z/XbE60dY0t1J28r541kfF7uPG76+wWLvjLXFGxb7Li0pKb6mKdjfbv58370kIcH3WawkVBNWgqivCZNdkpOXw5rNa1iVuYpmqc1ITtj+n/ZOlc2MGb42av58X/NUzBDRazev5dbvbmXEpBEc1+447j74bjrW71jiptdvWc/rU1/nmYnPMHPlTGol1qJ/6/50rt+ZLg260DkvjT1vfwgWLWLp8YcyY782TI9dw/QV01mwfgEbszayYesGNm71tzEuhn0a70Pfpn3p27QvvRv3pma1msxYOYPP//6cz//+nJ///Zlcy6V9vfZc1fsqzup6FtXj/WsyM1774zWu+vIqcvJyeODwB2hfrz1jF47ll4W/MG7RONZtWQdA6zqttw3Oe3CLg6mVWKvI15idm81/3v8PH//5MU8f/TQX97yY/475L4+Mf4TDWh7GyJNH8seEP2jXsx2nvn8qP/37E4P3HszDRz5MQmzCtrjWbF7D4j8nkNy0NQ1SGxU9GHBARlYGF4y6gHdnvMugzoN4/rjnt73GFRkrOPatY5m0eCJPfWZcmtnB/6t99FG4+moAPvvrM05+72Ra1m7JmLPG0GhdLqSnw2mnMWPtXxz5xpFkZGfw8vEv88GsD3hj6hs0S23G4/0fZ0C7Afy25DcuHHUh01ZM4+S2J/D4vx0ZkTOBO3O+oVvt9nww6BNa1ssfQHb8ovEM+X4I38z9ptjX1DClIYP3GcwlPS+hbnU/OHBWbhavTnmVe3++l/nr5gMw9KCh3NHvjhI/d4Vl5WYx/NfhDP9tOHPWzqFe9Xqc3+UcLp6eyIbYenQPvC/F2ZqzlXdnvMu9P93L7NWz6ZDWgdsOuI3/dPwPsTH5g4OaGUs2LuGjPz/ig1kf8OOCH8mzPFrVbkXfpn3p0bAHPRr2oNse3ahZrSazV83mvRkjeW/Km0xdNxuA/TenMWhjc07e2JS0nGpgRm6vnjzRfSu3jLub6vHVefropzm106khvfYpy6bQ58U+7L/n/nx5xpfbxQuwZvMa+r/Rn8lLJ3P8XsdzXrfz6N+6/7aanZ8W/MSxbx9L7cTafHv2t7Sq06rYfeVZHs9Pep4bxtxATl4O2XnZNKrRiDcHvsn+e+6/4xM2biTr1pt4efFohnVZxxI20L91f+495F7Wz15Pvz59/ECq997rLzgZHw+NG7Pi21HckT6UEZNH+IM1RqMajbis12Vc3PNi0pLTdtiVmTF79WzGzBnDmLlj+H7+92zK2gRA1wZdOa/beQzqPIi05DTWbVnH2R+dzad/fcqgzoMYceyI7X5rt+ZsZdyicSzftJz/dPxPsYMEz1kzh9ZPts7/zM6ahZ00kH9WzubHi46g7Re/0XdBLrEffeJbBQrJzs1m3KJxZGZnsle9vdgzdU9iXMx27/fMlTNJn59O+vx05q2bR63EWtRJqkPtxNrUSarDHil70Ltxb3o07EG1uB2vKDB/3XzGzBnDxCUT6du0LwPbD6RGtZJr4CuyJiysSZhzrj/wOBALvGBm9xVafiDwGNAFOM3M3i9tm0rCdm8VUTZmZb+0kpnx078/MWLSCH5Z+Mu2AypAzWo1iXWxrN2ydtu8+sn1aVm7JanVUqlZrSY1EmpQs1pNNudsZvyi8UxbMY08y8PhqJNUh9WbfVNz1wZdObrN0Rzd5mj2a7pfsXEuXL+QC0ZdwJi5Y7bN65DWgf2a7sd+TffjwGYH0qJ2i5BfX1ZuFiePPJlP//qUHg17MHnpZAbvPZhH+z9KXEzctnLJzs3mpm9u4pHxj9Btj240TGnIgvULWLBuARnZGdtts3p8dRokN6BBSgMAMrMz2Zy9mczsTNZtWUdmdib3HXYfN/S9YYfXmZGVwakjT+GzOV9wwmxHyt77sblxA7bkbGFzzmZ+XPAjXRt05cszvyzykj8L1i3giDeO4K/Vf5EQm8ANfW/glgNu2ZbogT9APDj2QYb9MIycvBxyLZezu57NM8c8s916Bf38789MWTYFMyPP8rZNTVObcsJeJ2xLSgvLzs3mtT9e46d/f+Kpo58qMUEtSZ7l8e3cb3l20rN88ucn5FouTZOa0r5Re5qnNqd5LT9l5WYxa9Us/lz1J7NWzWLOmjnkWi5dGnRhyIFDGNh+4HYHw+Is37Scj//8mNF/j2by0sks2bhk27IGyQ1YnrEcgL5N+3JKh1M4qf1JNE1tWuz2/lz1J+d8fA6/Lv6VLg260LpOa5qnNqdZrWY0S21GWnIaDv9ZcM6Rm5fLOR+fw+aczfx+ye/UT65f5HY3bN3A0PShvD71dVZlrmKPlD04q8tZdEzryKWfXcqeqXvyzdnf0KRmyRfUDlq0YRHXfnUtiXGJPN7/cWonlX4Fkc3Zm3nq16f438//Y+2WtexXdz+O7348Het3pOMqx55X3MbWPybz+JNncM/aUWRmZ3LZ3pcx5MAh/Lr4Vx6f8Dhj5o6hWmw1Tut0Gnuk7MHSTUtZunEpSzYuYcnGJdt+Y1rVbsXhLQ/nsJaHsTxjOS9PeZmJSyYSHxPPsW2PZeryqSxYv4BHjniEwfsM3qUrMRz15lFMXT6VJ496kq/nfM1Xf3/J/A0Lti1vUK0uJ3Q6mYHtB3Jw84PJzM7ky3++ZNRfo/j878+3/SkESIpLol29duxVby+ycrP4ccGPrMpcBcCeqXvSMa0jG7ZuYM3mNazdspY1m9eQlZsFQLXYavRq1Iv9mu7HXvX2YsLiCXwz95tt3Ueqx1cnMzuTpLgkTmx/Imd3OZtDWx66QzMrVJEkzDkXC/wFHA4sAn4DTjezmQXWaQ7UBK4HRikJk9JUlrLZsHUD01dMZ9ryaUxdPpVcy6VT/U50qt+Jjmkdi/wnW9DGrRv5dfGvjF04lrnr5rJ/0/3p37o/jWs2DjkGM+PTvz4lLiaOPk36hHSgKMnWnK0MHDmQr/75iiePepJL975027LC5fL+zPe55dtbSElI2XYAbZbajCY1m5CRncHyTctZnrGcFRkrWJGxAvA/kgWn49sdz6EtDy0cxjY5eTlc9+lg3pv9EYnVkkmKTyIxLpGkuCRa12nN4/0fJzWx+P5EKzNW8uSvT3JmlzNpW7dtsevNXjWbW7+7lcNbHs7FPS+uNNc+XbJxCa9OeZWvpn1FZnwm89fNZ2Xmym3L42LiaFOnDe3T2tO+Xnv6Nu1L/9b9Q0q+irNs0zJ+X/o7vy/7nVmrZrF3o705qf1JZfrc5uTl8MSEJ/h6ztfbEvjNOZuLXT/WxZJ+bnrRNVGFZOVm8fnfn/PKlFf47O/PyMnLoWuDrnx91tfFJnDlbf2W9Tw09iGemfAMq7NWb5ufkpBCEvGszFrLcW2P44HDH2Cventt99yZK2fy5IQneW3qa2TnZrNHyh40rNGQhil+6rZHNw5vdTgta7fcYb/TV0znlSmv8PrU14mPiefdk99lvz332+XXM2r2KI5/53gAaiTU4JAWh3BkqyM4aD5MaxTLh0u/47O/PiMjO4PUaqlkZGeQk5dDver1OKbNMQxoN4B61esxe9Vs/lz1J3+u/pNZK2cBcFDzgzio2UH0a96P5rWa77BvM2NFxoptNf2/LPyFSUsmkZ2XTY2EGhzc4mAOa3EYh7U8jHb12jFu4Then/o67854l3Vb1rFHyh7cf9j9nN317O22W1WSsD7AUDM7MvD4ZgAz+18R674CjFYSJqVR2URWbl4uKzJW0LBGw+3mq1yiV8GyycjKYP66+cTFxNGydkviY6P/4uhmxsrMlSxYt4A1m9dg2Lb5AC1rt6RdvXZl3u6KjBV89c9XHNfuuGKb4sMpPT2dLr27MHPlTGasmMGMlTNYtmkZl/S8pMQ/H+AT1VgXu1N/CHLzcolxMeX2ZyLP8nhj6hu0qNWCfZvsW+RnanP2ZsbMHcOnsz+lTlIdBrQbwL5N9t2h6bg8bM7ezLx182hbt22RtVzg/1B+9vdnvD71dc7vdj7HtTtuu+UVmYSF8+rTjYGFBR4vAnqHcX8iEmaxMbE7JGBSeSQnJJfazzHaOOeon1y/3Guq6ifX56yuZ5XrNsuqTlId9t9z/5Bq8QoqLrkIRXknPjEuZoeapMKS4pMY0G4AA9oNKNd9F7evDmkdSlynWlw1BrYfyMD2A8MeT2nCmYSVG+fcxcDFAA0aNCA9PT2s+9u0aVPY9yE7R2UTnVQu0UtlE51ULtGrIssmnEnYYqBgD8wmgXllZmYjgBHgmyPD3eyhppXopbKJTiqX6KWyiU4ql+hVkWWz8z0wS/cb0MY518I5lwCcBowK4/5EREREKo2wJWFmlgMMBr4CZgEjzWyGc26Yc24AgHNub+fcIuAU4Dnn3IxwxSMiIiISTcLaJ8zMPgc+LzTv9gL3f8M3U4qIiIjsVsLZHCkiIiIixVASJiIiIhIBSsJEREREIkBJmIiIiEgEKAkTERERiQAlYSIiIiIRoCRMREREJAKUhImIiIhEgDOzSMdQJs65lcCCMO+mHrAqzPuQnaOyiU4ql+ilsolOKpfoVd5l08zM0opaUOmSsIrgnJtoZr0iHYfsSGUTnVQu0UtlE51ULtGrIstGzZEiIiIiEaAkTERERCQClIQVbUSkA5BiqWyik8oleqlsopPKJXpVWNmoT5iIiIhIBKgmTERERCQClIQV4pzr75yb7Zz7xzl3U6Tj2V0555o65753zs10zs1wzl0VmF/HOTfGOfd34LZ2pGPdXTnnYp1zvzvnRgcet3DOTQh8d951ziVEOsbdjXOulnPufefcn865Wc65PvrORAfn3DWB37Lpzrm3nXOJ+s5EhnPuJefcCufc9ALzivyeOO+JQBlNdc71KM9YlIQV4JyLBYYDRwEdgNOdcx0iG9VuKwe4zsw6APsClwfK4ibgWzNrA3wbeCyRcRUwq8Dj+4FHzaw1sBa4ICJR7d4eB740s72Arvjy0XcmwpxzjYErgV5m1gmIBU5D35lIeQXoX2hecd+To4A2geli4JnyDERJ2Pb2Af4xs7lmlgW8Axwf4Zh2S2a21MwmB+5vxB9MGuPL49XAaq8CJ0QkwN2cc64JcAzwQuCxAw4B3g+sorKpYM65VOBA4EUAM8sys3XoOxMt4oAk51wcUB1Yir4zEWFmPwJrCs0u7ntyPPCaeeOBWs65huUVi5Kw7TUGFhZ4vCgwTyLIOdcc6A5MABqY2dLAomVAg0jFtZt7DPgvkBd4XBdYZ2Y5gcf67lS8FsBK4OVAM/ELzrlk9J2JODNbDDwE/ItPvtYDk9B3JpoU9z0Ja16gJEyimnMuBfgAuNrMNhRcZv7UXp3eW8Gcc8cCK8xsUqRjke3EAT2AZ8ysO5BBoaZHfWciI9C/6Hh8otwISGbH5jCJEhX5PVEStr3FQNMCj5sE5kkEOOfi8QnYm2b2YWD28mBVcOB2RaTi243tBwxwzs3HN9kfgu+LVCvQ1AL67kTCImCRmU0IPH4fn5TpOxN5hwHzzGylmWUDH+K/R/rORI/ividhzQuUhG3vN6BN4IyVBHzHyVERjmm3FOhj9CIwy8weKbBoFHBO4P45wCcVHdvuzsxuNrMmZtYc/x35zszOAL4HTg6sprKpYGa2DFjonGsXmHUoMBN9Z6LBv8C+zrnqgd+2YNnoOxM9ivuejALODpwluS+wvkCz5S7TYK2FOOeOxvd3iQVeMrN7IhvR7sk5tz/wEzCN/H5Ht+D7hY0E9gQWAP8xs8IdLKWCOOf6Adeb2bHOuZb4mrE6wO/AmWa2NYLh7Xacc93wJ0skAHOB8/B/tvWdiTDn3J3Aqfgzv38HLsT3LdJ3poI5594G+gH1gOXAHcDHFPE9CSTNT+GbjzOB88xsYrnFoiRMREREpOKpOVJEREQkApSEiYiIiESAkjARERGRCFASJiIiIhIBSsJEREREIkBJmIhUes65XOfclAJTuV2k2jnX3Dk3vby2JyISFFf6KiIiUW+zmXWLdBAiImWhmjARqbKcc/Odcw8456Y55351zrUOzG/unPvOOTfVOfetc27PwPwGzrmPnHN/BKa+gU3FOueed87NcM597ZxLCqx/pXNuZmA770ToZYpIJaUkTESqgqRCzZGnFli23sw640e9fiww70ngVTPrArwJPBGY/wTwg5l1xV93cUZgfhtguJl1BNYBJwXm3wR0D2zn/8Lz0kSkqtKI+SJS6TnnNplZShHz5wOHmNncwAXhl5lZXefcKqChmWUH5i81s3rOuZVAk4KXjnHONQfGmFmbwOMbgXgzu9s59yWwCX/Jk4/NbFOYX6qIVCGqCRORqs6KuV8WBa/nl0t+f9pjgOH4WrPfnHPqZysiIVMSJiJV3akFbscF7o8FTgvcPwN/sXiAb4FLAZxzsc651OI26pyLAZqa2ffAjUAqsENtnIhIcfSvTUSqgiTn3JQCj780s+AwFbWdc1PxtVmnB+ZdAbzsnLsBWAmcF5h/FTDCOXcBvsbrUmBpMfuMBd4IJGoOeMLM1pXT6xGR3YD6hIlIlRXoE9bLzFZFOhYRkcLUHCkiIiISAaoJExEREYkA1YSJiIiIRICSMBEREZEIUBImIiIiEgFKwkREREQiQEmYiIiISAQoCRMRERGJgP8HHmelS7cBX5gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(max_epoch), obj_values_gd, label=\"GD\", color=\"blue\")\n",
    "plt.plot(range(max_epoch), obj_values_sgd, label=\"SGD\", color=\"red\")\n",
    "plt.plot(range(max_epoch), obj_values_mbgd, label=\"MBGD\", color=\"green\")\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Objective Function Value\")\n",
    "plt.title(\"Comparison of GD, SGD, and MBGD\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Prediction\n",
    "### Compare the training and testing accuracy for logistic regression and regularized logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class label\n",
    "# Inputs:\n",
    "#     w: weights: d-by-1 matrix\n",
    "#     X: data: m-by-d matrix\n",
    "# Return:\n",
    "#     f: m-by-1 matrix, the predictions\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_1(w, X):\n",
    "    a = X @ w\n",
    "    \n",
    "    # Apply the sigmoid function to get probabilities\n",
    "    probabilities = 1 / (1 + np.exp(-a))\n",
    "    \n",
    "    # Predict class labels (1 if probability > 0.5, else -1)\n",
    "    predictions = np.where(probabilities >= 0.5, 1, -1)  # This maps the output to 1 and -1\n",
    "    \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 97.58%\n"
     ]
    }
   ],
   "source": [
    "# evaluate training error of logistric regression and regularized version\n",
    "y_train_pred = predict_1(w_optimized_1, x_train)\n",
    "\n",
    "train_accuracy = np.mean(y_train_pred == y_train)\n",
    "print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 98.25%\n"
     ]
    }
   ],
   "source": [
    "# evaluate testing error of logistric regression and regularized version\n",
    "y_test_pred = predict_1(w_optimized, x_test)\n",
    "test_accuracy = np.mean(y_test_pred == y_test)\n",
    "\n",
    "print(f\"Testing Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Parameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this section, you may try different combinations of parameters (regularization value, learning rate, etc) to see their effects on the model. (Open ended question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "λ=0, lr=0.01 | GD: 92.98% | SGD: 97.37% | MBGD: 98.25%\n",
      "λ=0, lr=0.1 | GD: 98.25% | SGD: 96.49% | MBGD: 98.25%\n",
      "λ=0.01, lr=0.01 | GD: 92.98% | SGD: 98.25% | MBGD: 98.25%\n",
      "λ=0.01, lr=0.1 | GD: 98.25% | SGD: 98.25% | MBGD: 98.25%\n"
     ]
    }
   ],
   "source": [
    "for lam in [0, 0.01]:\n",
    "    for lr in [0.01, 0.1]:\n",
    "        w0 = np.zeros((x_train.shape[1], 1))\n",
    "        w_gd, _ = gradient_descent(x_train, y_train, lam, lr, w0.copy(), 100)\n",
    "        w_sgd, _ = sgd(x_train, y_train, lam, lr, w0.copy(), 100)\n",
    "        w_mbgd, _ = mbgd(x_train, y_train, lam, lr, w0.copy(), 100, batch_size=32)\n",
    "        print(f\"λ={lam}, lr={lr} | GD: {np.mean(predict_1(w_gd, x_test)==y_test)*100:.2f}% | \"\n",
    "              f\"SGD: {np.mean(predict_1(w_sgd, x_test)==y_test)*100:.2f}% | \"\n",
    "              f\"MBGD: {np.mean(predict_1(w_mbgd, x_test)==y_test)*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
